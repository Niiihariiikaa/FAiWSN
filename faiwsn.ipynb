{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":267091,"sourceType":"datasetVersion","datasetId":111554}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import pandas as pd\n#df=pd.read_csv('/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv')\n#df2=pd.read_csv('/kaggle/input/unsw-nb15/UNSW-NB15_1.csv')\n#df.info()\n#df.head()\n#df2.info()\n#df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, LSTM, Flatten, Dropout\nfrom tensorflow.keras.utils import to_categorical\n\ndf = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\n\ndf.drop(columns=[\"id\"], inplace=True, errors=\"ignore\")\n\nX = df.drop(columns=[\"label\", \"attack_cat\"], errors=\"ignore\")\ny = df[\"label\"]\n\ncat_cols = X.select_dtypes(include=\"object\").columns\nfor col in cat_cols:\n    le = LabelEncoder()\n    X[col] = le.fit_transform(X[col].astype(str))\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nclient_data = np.array_split(X_scaled, 5)\nclient_labels = np.array_split(y.values, 5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:40:23.155889Z","iopub.execute_input":"2025-07-14T17:40:23.156238Z","iopub.status.idle":"2025-07-14T17:40:23.599888Z","shell.execute_reply.started":"2025-07-14T17:40:23.156216Z","shell.execute_reply":"2025-07-14T17:40:23.599156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model(input_shape):\n    model = Sequential()\n    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(LSTM(32, return_sequences=False))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:40:27.253314Z","iopub.execute_input":"2025-07-14T17:40:27.253617Z","iopub.status.idle":"2025-07-14T17:40:27.260220Z","shell.execute_reply.started":"2025-07-14T17:40:27.253595Z","shell.execute_reply":"2025-07-14T17:40:27.259136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nX_scaled = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))\n\nfor i in range(5):\n    client_data[i] = client_data[i].reshape((client_data[i].shape[0], client_data[i].shape[1], 1))\n\nglobal_model = create_model(input_shape=(X_scaled.shape[1], 1))\n\nglobal_weights = global_model.get_weights()\n\nEPOCHS = 1\nROUNDS = 3\n\nfor round_num in range(ROUNDS):\n    print(f\"\\nüåç Global Round {round_num+1}\")\n    local_weights = []\n    \n    for i in range(5):\n        print(f\"  üõ∞Ô∏è Client {i+1}\")\n        local_model = create_model(input_shape=(X_scaled.shape[1], 1))\n        local_model.set_weights(global_weights)\n        \n        local_model.fit(client_data[i], client_labels[i], epochs=EPOCHS, batch_size=32, verbose=0)\n        \n        local_weights.append(local_model.get_weights())\n\n    new_weights = []\n    for weights in zip(*local_weights):\n        new_weights.append(np.mean(weights, axis=0))\n    \n    global_weights = new_weights\n    global_model.set_weights(global_weights)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:40:32.039333Z","iopub.execute_input":"2025-07-14T17:40:32.040009Z","iopub.status.idle":"2025-07-14T17:42:14.162796Z","shell.execute_reply.started":"2025-07-14T17:40:32.039978Z","shell.execute_reply":"2025-07-14T17:42:14.161978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\ndf_test.drop(columns=[\"id\"], inplace=True, errors=\"ignore\")\ny_test = df_test[\"label\"]\nX_test = df_test.drop(columns=[\"label\", \"attack_cat\"], errors=\"ignore\")\n\n\nfor col in X_test.select_dtypes(include=\"object\").columns:\n    le = LabelEncoder()\n    X_test[col] = le.fit_transform(X_test[col].astype(str))\n\nX_test_scaled = scaler.transform(X_test)\nX_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n\nloss, acc = global_model.evaluate(X_test_scaled, y_test, verbose=0)\nprint(f\"\\n‚úÖ Final Model Accuracy on Test Set: {acc*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:42:56.684062Z","iopub.execute_input":"2025-07-14T17:42:56.684444Z","iopub.status.idle":"2025-07-14T17:43:17.238341Z","shell.execute_reply.started":"2025-07-14T17:42:56.684418Z","shell.execute_reply":"2025-07-14T17:43:17.237514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ny_pred = (global_model.predict(X_test_scaled) > 0.5).astype(int)\n\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Normal','Attack'], yticklabels=['Normal','Attack'])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:43:23.045011Z","iopub.execute_input":"2025-07-14T17:43:23.045289Z","iopub.status.idle":"2025-07-14T17:43:37.691040Z","shell.execute_reply.started":"2025-07-14T17:43:23.045269Z","shell.execute_reply":"2025-07-14T17:43:37.690077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --quiet tensorflow_federated\n\nimport tensorflow_federated as tff\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:43:41.060670Z","iopub.execute_input":"2025-07-14T17:43:41.061389Z","iopub.status.idle":"2025-07-14T17:43:47.005739Z","shell.execute_reply.started":"2025-07-14T17:43:41.061341Z","shell.execute_reply":"2025-07-14T17:43:47.004390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:43:51.490656Z","iopub.execute_input":"2025-07-14T17:43:51.491591Z","iopub.status.idle":"2025-07-14T17:43:51.496579Z","shell.execute_reply.started":"2025-07-14T17:43:51.491558Z","shell.execute_reply":"2025-07-14T17:43:51.495605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\ndf.drop(columns=[\"id\"], inplace=True, errors=\"ignore\")\n\nX = df.drop(columns=[\"label\", \"attack_cat\"], errors=\"ignore\")\ny = df[\"label\"]\n\nfor col in X.select_dtypes(include=\"object\").columns:\n    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:44:06.450658Z","iopub.execute_input":"2025-07-14T17:44:06.451814Z","iopub.status.idle":"2025-07-14T17:44:06.931810Z","shell.execute_reply.started":"2025-07-14T17:44:06.451753Z","shell.execute_reply":"2025-07-14T17:44:06.930894Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf1 = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW-NB15_1.csv\")\ndf2 = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW-NB15_2.csv\")\ndf3 = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW-NB15_3.csv\")\ndf4 = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW-NB15_4.csv\")\n\ndf_full = pd.concat([df1, df2, df3, df4], ignore_index=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:44:11.054570Z","iopub.execute_input":"2025-07-14T17:44:11.055342Z","iopub.status.idle":"2025-07-14T17:44:34.047005Z","shell.execute_reply.started":"2025-07-14T17:44:11.055311Z","shell.execute_reply":"2025-07-14T17:44:34.046205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef sliding_window(X, y, window_size=20):\n    Xs, ys = [], []\n    for i in range(len(X) - window_size):\n        Xs.append(X[i:i+window_size])\n        ys.append(y[i+window_size])\n    return np.array(Xs), np.array(ys)\n\n# üîÄ Split into 5 clients (WSN nodes)\nX_parts = np.array_split(X_scaled, 5)\ny_parts = np.array_split(y.values, 5)\n\nclient_data, client_labels = [], []\nfor i in range(5):\n    X_seq, y_seq = sliding_window(X_parts[i], y_parts[i])\n    if len(X_seq.shape) == 2:\n        X_seq = X_seq.reshape((X_seq.shape[0], 10, -1))\n    client_data.append(X_seq)\n    client_labels.append(y_seq)\n\n# üîç Check shape\nfor i in range(5):\n    print(f\"Client {i+1} data: {client_data[i].shape}, labels: {client_labels[i].shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:45:11.379707Z","iopub.execute_input":"2025-07-14T17:45:11.380624Z","iopub.status.idle":"2025-07-14T17:45:11.684713Z","shell.execute_reply.started":"2025-07-14T17:45:11.380594Z","shell.execute_reply":"2025-07-14T17:45:11.683767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_better_model(input_shape, num_classes):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling1D(2),\n\n        tf.keras.layers.Conv1D(128, 3, activation='relu'),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling1D(2),\n\n        tf.keras.layers.LSTM(64, return_sequences=True),\n        tf.keras.layers.LSTM(32),\n\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:45:48.791329Z","iopub.execute_input":"2025-07-14T17:45:48.792030Z","iopub.status.idle":"2025-07-14T17:45:48.798551Z","shell.execute_reply.started":"2025-07-14T17:45:48.792004Z","shell.execute_reply":"2025-07-14T17:45:48.797553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"global_model = create_model(client_data[0].shape[1:])\nglobal_weights = global_model.get_weights()\n\nROUNDS = 5\nEPOCHS = 3\n\nfor rnd in range(ROUNDS):\n    print(f\"\\nüåç Federated Round {rnd+1}\")\n    local_weights = []\n    for i in range(5):\n        print(f\"  üõ∞Ô∏è Client {i+1}\")\n        model = create_model(client_data[i].shape[1:])\n        model.set_weights(global_weights)\n        model.fit(client_data[i], client_labels[i], epochs=EPOCHS, batch_size=32, verbose=0)\n        local_weights.append(model.get_weights())\n    \n    # üßÆ FedAvg aggregation\n    new_weights = []\n    for weights in zip(*local_weights):\n        new_weights.append(np.mean(weights, axis=0))\n    \n    global_weights = new_weights\n    global_model.set_weights(global_weights)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:45:53.467886Z","iopub.execute_input":"2025-07-14T17:45:53.468214Z","iopub.status.idle":"2025-07-14T17:51:29.456182Z","shell.execute_reply.started":"2025-07-14T17:45:53.468189Z","shell.execute_reply":"2025-07-14T17:51:29.455289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndf_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\ndf_test.drop(columns=[\"id\"], inplace=True, errors=\"ignore\")\nX_test = df_test.drop(columns=[\"label\", \"attack_cat\"], errors=\"ignore\")\ny_test = df_test[\"label\"]\n\nfor col in X_test.select_dtypes(include=\"object\").columns:\n    X_test[col] = LabelEncoder().fit_transform(X_test[col].astype(str))\nX_test_scaled = scaler.transform(X_test)\n\nX_test_seq, y_test_seq = sliding_window(X_test_scaled, y_test.values)\nX_test_seq = X_test_seq.reshape((X_test_seq.shape[0], X_test_seq.shape[1], -1))\n\n\ny_pred = (global_model.predict(X_test_seq) > 0.5).astype(int)\n\n# üìä Report\nprint(\"üìä Classification Report:\")\nprint(classification_report(y_test_seq, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:51:46.980930Z","iopub.execute_input":"2025-07-14T17:51:46.981663Z","iopub.status.idle":"2025-07-14T17:52:05.283815Z","shell.execute_reply.started":"2025-07-14T17:51:46.981627Z","shell.execute_reply":"2025-07-14T17:52:05.282839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(y_test_seq, y_pred)\nplt.figure(figsize=(6,4))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", xticklabels=[\"Normal\", \"Attack\"], yticklabels=[\"Normal\", \"Attack\"])\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:52:11.334555Z","iopub.execute_input":"2025-07-14T17:52:11.334917Z","iopub.status.idle":"2025-07-14T17:52:11.532058Z","shell.execute_reply.started":"2025-07-14T17:52:11.334892Z","shell.execute_reply":"2025-07-14T17:52:11.530987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nround_accuracies = []\nround_losses = []\n\nROUNDS = 5\nEPOCHS = 5\n\nfor rnd in range(ROUNDS):\n    print(f\"\\nüåç Federated Round {rnd+1}\")\n    local_weights = []\n    acc_list, loss_list = [], []\n    \n    for i in range(5):\n        print(f\"  üõ∞Ô∏è Client {i+1}\")\n        model = create_model(client_data[i].shape[1:])\n        model.set_weights(global_weights)\n\n        history = model.fit(client_data[i], client_labels[i],\n                            epochs=EPOCHS, batch_size=32,\n                            verbose=0)\n\n        acc_list.append(history.history['accuracy'][-1])\n        loss_list.append(history.history['loss'][-1])\n        local_weights.append(model.get_weights())\n\n    # FedAvg aggregation\n    new_weights = []\n    for weights in zip(*local_weights):\n        new_weights.append(np.mean(weights, axis=0))\n\n    global_weights = new_weights\n    global_model.set_weights(global_weights)\n\n    # Save round avg metrics\n    round_accuracies.append(np.mean(acc_list))\n    round_losses.append(np.mean(loss_list))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T17:52:42.432529Z","iopub.execute_input":"2025-07-14T17:52:42.433062Z","iopub.status.idle":"2025-07-14T18:00:59.218579Z","shell.execute_reply.started":"2025-07-14T17:52:42.433037Z","shell.execute_reply":"2025-07-14T18:00:59.217910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# üìà Accuracy and Loss over Federated Rounds\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1, ROUNDS + 1), round_accuracies, marker='o', color='green')\nplt.title(\"Federated Round Accuracy\")\nplt.xlabel(\"Round\")\nplt.ylabel(\"Accuracy\")\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, ROUNDS + 1), round_losses, marker='o', color='red')\nplt.title(\"Federated Round Loss\")\nplt.xlabel(\"Round\")\nplt.ylabel(\"Loss\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:01:06.120551Z","iopub.execute_input":"2025-07-14T18:01:06.121042Z","iopub.status.idle":"2025-07-14T18:01:06.549504Z","shell.execute_reply.started":"2025-07-14T18:01:06.121011Z","shell.execute_reply":"2025-07-14T18:01:06.548697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nfrom IPython.display import clear_output, display\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:01:12.970114Z","iopub.execute_input":"2025-07-14T18:01:12.970405Z","iopub.status.idle":"2025-07-14T18:01:12.974741Z","shell.execute_reply.started":"2025-07-14T18:01:12.970374Z","shell.execute_reply":"2025-07-14T18:01:12.973923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a real-time stream using a portion of test data\nstream_length = 30  # adjust for demo\nstream_X = X_test_seq[:stream_length]\nstream_y = y_test_seq[:stream_length]\n\n# Run simulation\nfor i in range(stream_length):\n    clear_output(wait=True)\n    \n    # Predict current sample\n    pred = global_model.predict(stream_X[i].reshape(1, 10, -1), verbose=0)\n    pred_label = int(pred[0][0] > 0.5)\n    \n    # Actual label\n    actual = stream_y[i]\n    \n    # Display\n    print(f\"üì∂ Time: {i+1}\")\n    print(\"üîç Sample (first 3 timesteps):\")\n    print(np.round(stream_X[i][:3].flatten(), 2))  # preview\n    print(f\"\\nüß† Prediction: {'‚ùå Attack' if pred_label==1 else '‚úÖ Normal'}\")\n    print(f\"üõ°Ô∏è  Actual:     {'‚ùå Attack' if actual==1 else '‚úÖ Normal'}\")\n    \n    # Pause like real stream\n    time.sleep(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:01:16.493434Z","iopub.execute_input":"2025-07-14T18:01:16.494096Z","iopub.status.idle":"2025-07-14T18:01:33.722112Z","shell.execute_reply.started":"2025-07-14T18:01:16.494068Z","shell.execute_reply":"2025-07-14T18:01:33.720775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load training data\ndf = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\ndf.drop(columns=[\"id\"], inplace=True, errors=\"ignore\")\n\n# Use attack_cat as target\nX = df.drop(columns=[\"label\", \"attack_cat\"], errors=\"ignore\")\ny = df[\"attack_cat\"].fillna(\"Normal\")  # Fill blanks as Normal\n\n# Label encoding\nfor col in X.select_dtypes(include=\"object\").columns:\n    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\nnum_classes = len(label_encoder.classes_)  # Needed for final Dense layer\n\n# One-hot encode the output\ny_cat = tf.keras.utils.to_categorical(y_encoded, num_classes=num_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:01:39.628896Z","iopub.execute_input":"2025-07-14T18:01:39.629208Z","iopub.status.idle":"2025-07-14T18:01:40.112723Z","shell.execute_reply.started":"2025-07-14T18:01:39.629188Z","shell.execute_reply":"2025-07-14T18:01:40.111906Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**MULTI CLASS SETUP**","metadata":{}},{"cell_type":"code","source":"# Load dataset (use merged one if applicable)\ndf = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\ndf.drop(columns=[\"id\", \"label\"], inplace=True, errors=\"ignore\")\n\n# Fill NaNs in 'attack_cat' with 'Normal'\ndf[\"attack_cat\"] = df[\"attack_cat\"].fillna(\"Normal\")\n\nX = df.drop(columns=[\"attack_cat\"])\ny = df[\"attack_cat\"]\n\n# Encode categorical features in X\nfor col in X.select_dtypes(include='object').columns:\n    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n\n# Encode target labels\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\ny_cat = to_categorical(y_encoded)\n\nnum_classes = len(label_encoder.classes_)\nprint(\"Classes:\", label_encoder.classes_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:01:43.880146Z","iopub.execute_input":"2025-07-14T18:01:43.880443Z","iopub.status.idle":"2025-07-14T18:01:44.303130Z","shell.execute_reply.started":"2025-07-14T18:01:43.880419Z","shell.execute_reply":"2025-07-14T18:01:44.302302Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalize features\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\ndef sliding_window(X, y, window_size=10):\n    Xs, ys = [], []\n    for i in range(len(X) - window_size):\n        Xs.append(X[i:i+window_size])\n        ys.append(y[i+window_size])\n    return np.array(Xs), np.array(ys)\n\n# Partition into clients\nX_parts = np.array_split(X_scaled, 5)\ny_parts = np.array_split(y_cat, 5)\n\nclient_data, client_labels = [], []\nfor i in range(5):\n    X_seq, y_seq = sliding_window(X_parts[i], y_parts[i], window_size=10)\n    client_data.append(X_seq)\n    client_labels.append(y_seq)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:01:48.331613Z","iopub.execute_input":"2025-07-14T18:01:48.331946Z","iopub.status.idle":"2025-07-14T18:01:48.643459Z","shell.execute_reply.started":"2025-07-14T18:01:48.331921Z","shell.execute_reply":"2025-07-14T18:01:48.642555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_multiclass_model(input_shape, num_classes):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Conv1D(64, 3, activation='relu', input_shape=input_shape),\n        tf.keras.layers.BatchNormalization(),\n        tf.keras.layers.MaxPooling1D(2),\n        \n        tf.keras.layers.LSTM(64, return_sequences=True),\n        tf.keras.layers.LSTM(32),\n\n        tf.keras.layers.Dense(128, activation='relu'),\n        tf.keras.layers.Dropout(0.4),\n        tf.keras.layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:01:55.825729Z","iopub.execute_input":"2025-07-14T18:01:55.826629Z","iopub.status.idle":"2025-07-14T18:01:55.832445Z","shell.execute_reply.started":"2025-07-14T18:01:55.826602Z","shell.execute_reply":"2025-07-14T18:01:55.831339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"global_model = create_multiclass_model(client_data[0].shape[1:], num_classes)\nglobal_weights = global_model.get_weights()\n\nROUNDS = 5\nEPOCHS = 2\n\nfor rnd in range(ROUNDS):\n    print(f\"\\nüåç Federated Round {rnd+1}\")\n    local_weights = []\n    \n    for i in range(5):\n        model = create_multiclass_model(client_data[i].shape[1:], num_classes)\n        model.set_weights(global_weights)\n        model.fit(client_data[i], client_labels[i], epochs=EPOCHS, batch_size=32, verbose=0)\n        local_weights.append(model.get_weights())\n\n    # FedAvg\n    global_weights = [np.mean(w, axis=0) for w in zip(*local_weights)]\n    global_model.set_weights(global_weights)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:02:01.156117Z","iopub.execute_input":"2025-07-14T18:02:01.156452Z","iopub.status.idle":"2025-07-14T18:07:59.132918Z","shell.execute_reply.started":"2025-07-14T18:02:01.156416Z","shell.execute_reply":"2025-07-14T18:07:59.132175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"global_model = create_multiclass_model(client_data[0].shape[1:], num_classes)\nglobal_weights = global_model.get_weights()\n\nROUNDS = 5\nEPOCHS = 2\n\nfor rnd in range(ROUNDS):\n    print(f\"\\nüåç Federated Round {rnd+1}\")\n    local_weights = []\n    \n    for i in range(5):\n        model = create_multiclass_model(client_data[i].shape[1:], num_classes)\n        model.set_weights(global_weights)\n        model.fit(client_data[i], client_labels[i], epochs=EPOCHS, batch_size=32, verbose=0)\n        local_weights.append(model.get_weights())\n\n    # FedAvg\n    global_weights = [np.mean(w, axis=0) for w in zip(*local_weights)]\n    global_model.set_weights(global_weights)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:08:28.965303Z","iopub.execute_input":"2025-07-14T18:08:28.965991Z","iopub.status.idle":"2025-07-14T18:08:44.896683Z","shell.execute_reply.started":"2025-07-14T18:08:28.965960Z","shell.execute_reply":"2025-07-14T18:08:44.895010Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test data\ndf_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\ndf_test.drop(columns=[\"id\", \"label\"], inplace=True, errors=\"ignore\")\ndf_test[\"attack_cat\"] = df_test[\"attack_cat\"].fillna(\"Normal\")\n\nX_test = df_test.drop(columns=[\"attack_cat\"])\ny_test = df_test[\"attack_cat\"]\n\nfor col in X_test.select_dtypes(include='object').columns:\n    X_test[col] = LabelEncoder().fit_transform(X_test[col].astype(str))\n\nX_test_scaled = scaler.transform(X_test)\n\n# Encode labels\ny_test_encoded = label_encoder.transform(y_test)\ny_test_cat = to_categorical(y_test_encoded, num_classes=num_classes)\n\n# Sliding window\nX_test_seq, y_test_seq = sliding_window(X_test_scaled, y_test_cat, window_size=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:08:55.446143Z","iopub.execute_input":"2025-07-14T18:08:55.446956Z","iopub.status.idle":"2025-07-14T18:08:56.909051Z","shell.execute_reply.started":"2025-07-14T18:08:55.446918Z","shell.execute_reply":"2025-07-14T18:08:56.908099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\n# Step 1: Convert from one-hot to integer labels\ny_flat = [np.argmax(y, axis=1) for y in client_labels]\n\n# Step 2: Compute weights using all clients‚Äô data (optional: average weights)\ny_all = np.concatenate(y_flat)\nclasses = np.unique(y_all)\n\nclass_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_all)\nclass_weights_dict = dict(zip(classes, class_weights))\n\nprint(\"‚úÖ Computed class weights:\")\nfor i, w in class_weights_dict.items():\n    print(f\"{i}: {w:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:09:00.597577Z","iopub.execute_input":"2025-07-14T18:09:00.597953Z","iopub.status.idle":"2025-07-14T18:09:00.639421Z","shell.execute_reply.started":"2025-07-14T18:09:00.597916Z","shell.execute_reply":"2025-07-14T18:09:00.638448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"global_model = create_multiclass_model(client_data[0].shape[1:], num_classes)\nglobal_weights = global_model.get_weights()\n\nROUNDS = 5\nEPOCHS = 2\n\nfor rnd in range(ROUNDS):\n    print(f\"\\nüåç Federated Round {rnd+1}\")\n    local_weights = []\n\n    for i in range(len(client_data)):\n        print(f\"  üß† Training client {i}\")\n        model = create_multiclass_model(client_data[i].shape[1:], num_classes)\n        model.set_weights(global_weights)\n        \n        y_int = np.argmax(client_labels[i], axis=1)\n        model.fit(client_data[i], client_labels[i],\n                  epochs=EPOCHS,\n                  batch_size=32,\n                  class_weight=class_weights_dict,\n                  verbose=0)\n        \n        local_weights.append(model.get_weights())\n\n    # FedAvg\n    global_weights = [np.mean(w, axis=0) for w in zip(*local_weights)]\n    global_model.set_weights(global_weights)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:09:11.137632Z","iopub.execute_input":"2025-07-14T18:09:11.138328Z","iopub.status.idle":"2025-07-14T18:14:55.610089Z","shell.execute_reply.started":"2025-07-14T18:09:11.138303Z","shell.execute_reply":"2025-07-14T18:14:55.609448Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install focal loss\n!pip install keras-losses\n\nfrom keras_losses import SparseCategoricalFocalLoss\nloss_fn = SparseCategoricalFocalLoss(gamma=2.0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:15:33.138870Z","iopub.execute_input":"2025-07-14T18:15:33.139195Z","iopub.status.idle":"2025-07-14T18:15:35.071476Z","shell.execute_reply.started":"2025-07-14T18:15:33.139171Z","shell.execute_reply":"2025-07-14T18:15:35.069857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Predictions\ny_pred = global_model.predict(X_test_seq, verbose=0)\ny_pred_labels = np.argmax(y_pred, axis=1)\ny_true_labels = np.argmax(y_test_seq, axis=1)\n\n# Classification Report\nprint(\"üîç Classification Report:\\n\")\nprint(classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:15:39.669247Z","iopub.execute_input":"2025-07-14T18:15:39.669583Z","iopub.status.idle":"2025-07-14T18:15:56.398131Z","shell.execute_reply.started":"2025-07-14T18:15:39.669555Z","shell.execute_reply":"2025-07-14T18:15:56.397247Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import clear_output\nimport time\n\nstream_length = 30\nstream_X = X_test_seq[:stream_length]\nstream_y = y_test_seq[:stream_length]\n\nfor i in range(stream_length):\n    clear_output(wait=True)\n    \n    pred = global_model.predict(stream_X[i].reshape(1, 10, -1), verbose=0)\n    pred_class = label_encoder.inverse_transform([np.argmax(pred)])\n    actual_class = label_encoder.inverse_transform([np.argmax(stream_y[i])])\n    \n    print(f\"üì∂ Time: {i+1}\")\n    print(\"üîç Sample (first 3 timesteps):\")\n    print(np.round(stream_X[i][:3].flatten(), 2))\n    \n    print(f\"\\nüß† Prediction: {'‚ùå' if pred_class[0]!='Normal' else '‚úÖ'} {pred_class[0]}\")\n    print(f\"üõ°Ô∏è  Actual:    {'‚ùå' if actual_class[0]!='Normal' else '‚úÖ'} {actual_class[0]}\")\n    \n    time.sleep(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:16:05.237062Z","iopub.execute_input":"2025-07-14T18:16:05.237372Z","iopub.status.idle":"2025-07-14T18:16:16.877214Z","shell.execute_reply.started":"2025-07-14T18:16:05.237348Z","shell.execute_reply":"2025-07-14T18:16:16.876111Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**MULTI CLASS**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow as tf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:16:21.880380Z","iopub.execute_input":"2025-07-14T18:16:21.881372Z","iopub.status.idle":"2025-07-14T18:16:21.899089Z","shell.execute_reply.started":"2025-07-14T18:16:21.881343Z","shell.execute_reply":"2025-07-14T18:16:21.898189Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\ndf = df.drop(columns=[\"id\", \"label\"], errors=\"ignore\")\ndf[\"attack_cat\"] = df[\"attack_cat\"].fillna(\"Normal\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:16:25.198517Z","iopub.execute_input":"2025-07-14T18:16:25.198868Z","iopub.status.idle":"2025-07-14T18:16:25.572583Z","shell.execute_reply.started":"2025-07-14T18:16:25.198841Z","shell.execute_reply":"2025-07-14T18:16:25.571921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = df.drop(columns=[\"attack_cat\"])\ny = df[\"attack_cat\"]\n\nfor col in X.select_dtypes(include=\"object\").columns:\n    X[col] = LabelEncoder().fit_transform(X[col].astype(str))\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\ny_categorical = to_categorical(y_encoded)\nnum_classes = y_categorical.shape[1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:16:32.660308Z","iopub.execute_input":"2025-07-14T18:16:32.660617Z","iopub.status.idle":"2025-07-14T18:16:32.824582Z","shell.execute_reply.started":"2025-07-14T18:16:32.660594Z","shell.execute_reply":"2025-07-14T18:16:32.823768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sliding_window(X, y, window_size=10):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - window_size):\n        X_seq.append(X[i:i+window_size])\n        y_seq.append(y[i+window_size])\n    return np.array(X_seq), np.array(y_seq)\n\nX_seq, y_seq = sliding_window(X_scaled, y_categorical)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:16:37.037657Z","iopub.execute_input":"2025-07-14T18:16:37.038441Z","iopub.status.idle":"2025-07-14T18:16:37.274721Z","shell.execute_reply.started":"2025-07-14T18:16:37.038413Z","shell.execute_reply":"2025-07-14T18:16:37.274061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_seq_int = np.argmax(y_seq, axis=1)\nX_oversampled, y_oversampled = [], []\n\nunique_classes, counts = np.unique(y_seq_int, return_counts=True)\nmax_count = max(counts)\n\nfor cls in unique_classes:\n    idx = np.where(y_seq_int == cls)[0]\n    X_cls, y_cls = X_seq[idx], y_seq[idx]\n    \n    reps = max_count // len(X_cls)\n    rem = max_count % len(X_cls)\n    \n    X_res = np.concatenate([X_cls]*reps + [X_cls[:rem]])\n    y_res = np.concatenate([y_cls]*reps + [y_cls[:rem]])\n    \n    X_oversampled.append(X_res)\n    y_oversampled.append(y_res)\n\nX_bal = np.concatenate(X_oversampled)\ny_bal = np.concatenate(y_oversampled)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:16:39.906871Z","iopub.execute_input":"2025-07-14T18:16:39.907185Z","iopub.status.idle":"2025-07-14T18:16:40.863485Z","shell.execute_reply.started":"2025-07-14T18:16:39.907165Z","shell.execute_reply":"2025-07-14T18:16:40.862539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_model(input_shape, num_classes):\n    inp = Input(shape=input_shape)\n    x = Conv1D(64, kernel_size=3, activation='relu', padding='same')(inp)\n    x = MaxPooling1D(pool_size=2)(x)\n    x = Bidirectional(LSTM(64))(x)\n    x = Dropout(0.5)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    out = Dense(num_classes, activation='softmax')(x)\n    return Model(inp, out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:16:50.149032Z","iopub.execute_input":"2025-07-14T18:16:50.149658Z","iopub.status.idle":"2025-07-14T18:16:50.154720Z","shell.execute_reply.started":"2025-07-14T18:16:50.149634Z","shell.execute_reply":"2025-07-14T18:16:50.154043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sparse_categorical_focal_loss(gamma=2.0, alpha=0.25):\n    def loss_fn(y_true, y_pred):\n        y_true = tf.cast(tf.argmax(y_true, axis=1), tf.int32)\n        y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n        y_true_one_hot = tf.one_hot(y_true, depth=y_pred.shape[1])\n        cross_entropy = -y_true_one_hot * tf.math.log(y_pred)\n        weights = alpha * tf.math.pow(1 - y_pred, gamma)\n        loss = weights * cross_entropy\n        return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n    return loss_fn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:16:55.156371Z","iopub.execute_input":"2025-07-14T18:16:55.156671Z","iopub.status.idle":"2025-07-14T18:16:55.162579Z","shell.execute_reply.started":"2025-07-14T18:16:55.156648Z","shell.execute_reply":"2025-07-14T18:16:55.161491Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = create_model(input_shape=X_bal.shape[1:], num_classes=num_classes)\nmodel.compile(optimizer=Adam(0.001), loss=sparse_categorical_focal_loss(), metrics=['accuracy'])\n\nhistory = model.fit(X_bal, y_bal, \n                    epochs=10, \n                    batch_size=64, \n                    validation_split=0.2, \n                    verbose=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:17:02.216777Z","iopub.execute_input":"2025-07-14T18:17:02.217134Z","iopub.status.idle":"2025-07-14T18:24:42.678491Z","shell.execute_reply.started":"2025-07-14T18:17:02.217111Z","shell.execute_reply":"2025-07-14T18:24:42.677596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\ndf_test = df_test.drop(columns=[\"id\", \"label\"], errors='ignore')\ndf_test[\"attack_cat\"] = df_test[\"attack_cat\"].fillna(\"Normal\")\n\nX_test = df_test.drop(columns=[\"attack_cat\"])\ny_test = df_test[\"attack_cat\"]\n\nfor col in X_test.select_dtypes(include='object').columns:\n    X_test[col] = LabelEncoder().fit_transform(X_test[col].astype(str))\n\nX_test_scaled = scaler.transform(X_test)\ny_test_encoded = label_encoder.transform(y_test)\ny_test_cat = to_categorical(y_test_encoded, num_classes=num_classes)\n\nX_test_seq, y_test_seq = sliding_window(X_test_scaled, y_test_cat)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:25:05.805513Z","iopub.execute_input":"2025-07-14T18:25:05.806424Z","iopub.status.idle":"2025-07-14T18:25:07.296769Z","shell.execute_reply.started":"2025-07-14T18:25:05.806397Z","shell.execute_reply":"2025-07-14T18:25:07.296003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict(X_test_seq, verbose=0)\ny_pred_labels = np.argmax(y_pred, axis=1)\ny_true_labels = np.argmax(y_test_seq, axis=1)\n\nprint(\"üîç Classification Report:\\n\")\nprint(classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_))\n\ncm = confusion_matrix(y_true_labels, y_pred_labels)\nplt.figure(figsize=(12, 8))\nsns.heatmap(cm, annot=True, fmt=\"d\",\n            xticklabels=label_encoder.classes_,\n            yticklabels=label_encoder.classes_,\n            cmap=\"Blues\")\nplt.title(\"üìä Confusion Matrix - Multiclass IDS\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:25:10.279996Z","iopub.execute_input":"2025-07-14T18:25:10.280956Z","iopub.status.idle":"2025-07-14T18:25:27.866081Z","shell.execute_reply.started":"2025-07-14T18:25:10.280927Z","shell.execute_reply":"2025-07-14T18:25:27.865243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    sample = X_test_seq[i].reshape(1, 10, -1)\n    pred = model.predict(sample, verbose=0)\n    pred_class = label_encoder.inverse_transform([np.argmax(pred)])[0]\n    true_class = label_encoder.inverse_transform([np.argmax(y_test_seq[i])])[0]\n    confidence = np.max(pred)\n\n    print(f\"\\nüì∂ Time: {i+1}\")\n    print(f\"üîç Sample (first 3 timesteps):\\n{np.round(sample[0][:3].flatten(), 2)}\")\n    print(f\"\\nüß† Prediction: {'‚úÖ' if pred_class == true_class else '‚ùå'} {pred_class} ({confidence:.2f})\")\n    print(f\"üõ°Ô∏è  Actual:     ‚úÖ {true_class}\")\n\n    if confidence < 0.4:\n        print(\"‚ö†Ô∏è Low confidence ‚Äî possible unknown attack\")\n    time.sleep(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:25:49.781477Z","iopub.execute_input":"2025-07-14T18:25:49.782267Z","iopub.status.idle":"2025-07-14T18:25:58.079508Z","shell.execute_reply.started":"2025-07-14T18:25:49.782240Z","shell.execute_reply":"2025-07-14T18:25:58.078371Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> **ADAPTIVE**","metadata":{}},{"cell_type":"code","source":"adaptive_buffer_X = []\nadaptive_buffer_y = []\n\nCONFIDENCE_THRESHOLD = 0.6     # If model confidence < this, retrain\nMISCLASSIFY_TRIGGER = True     # Also retrain on wrong predictions\nBATCH_UPDATE_INTERVAL = 10     # Retrain after 10 samples in buffer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:26:04.972724Z","iopub.execute_input":"2025-07-14T18:26:04.973539Z","iopub.status.idle":"2025-07-14T18:26:04.977756Z","shell.execute_reply.started":"2025-07-14T18:26:04.973512Z","shell.execute_reply":"2025-07-14T18:26:04.976739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def adaptive_update(model, buffer_X, buffer_y, label_encoder):\n    if not buffer_X:\n        return model\n    \n    X_adapt = np.array(buffer_X)\n    y_adapt = np.array(buffer_y)\n    \n    # One-hot encode for training\n    y_adapt_cat = to_categorical(label_encoder.transform(y_adapt), num_classes=num_classes)\n\n    print(f\"\\nüîÅ Updating model with {len(X_adapt)} new samples...\")\n    model.fit(X_adapt, y_adapt_cat, epochs=2, batch_size=8, verbose=0)\n    print(\"‚úÖ Model updated.\\n\")\n\n    # Clear buffer\n    buffer_X.clear()\n    buffer_y.clear()\n    \n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:26:11.832221Z","iopub.execute_input":"2025-07-14T18:26:11.832501Z","iopub.status.idle":"2025-07-14T18:26:11.838363Z","shell.execute_reply.started":"2025-07-14T18:26:11.832483Z","shell.execute_reply":"2025-07-14T18:26:11.837301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(X_test_seq)):\n    sample = X_test_seq[i].reshape(1, 10, -1)\n    true_class = label_encoder.inverse_transform([np.argmax(y_test_seq[i])])[0]\n\n    # Predict\n    pred = model.predict(sample, verbose=0)\n    pred_class = label_encoder.inverse_transform([np.argmax(pred)])[0]\n    confidence = np.max(pred)\n\n    print(f\"\\nüì∂ Time: {i+1}\")\n    print(f\"üß† Prediction: {'‚úÖ' if pred_class == true_class else '‚ùå'} {pred_class} ({confidence:.2f})\")\n    print(f\"üõ°Ô∏è  Actual:     ‚úÖ {true_class}\")\n    \n    if confidence < CONFIDENCE_THRESHOLD or (MISCLASSIFY_TRIGGER and pred_class != true_class):\n        print(\"‚ö†Ô∏è Low confidence or misclassified ‚Äî adding to update buffer.\")\n        adaptive_buffer_X.append(sample[0])\n        adaptive_buffer_y.append(true_class)\n\n    # Update model every N samples\n    if len(adaptive_buffer_X) >= BATCH_UPDATE_INTERVAL:\n        model = adaptive_update(model, adaptive_buffer_X, adaptive_buffer_y, label_encoder)\n\n    time.sleep(0.5)  # Simulated delay to mimic real-time\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:26:21.336751Z","iopub.execute_input":"2025-07-14T18:26:21.337137Z","iopub.status.idle":"2025-07-14T18:26:40.281914Z","shell.execute_reply.started":"2025-07-14T18:26:21.337110Z","shell.execute_reply":"2025-07-14T18:26:40.280648Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Re-evaluate after adaptive training\nfinal_preds = model.predict(X_test_seq, verbose=0)\nfinal_pred_labels = np.argmax(final_preds, axis=1)\ntrue_labels = np.argmax(y_test_seq, axis=1)\n\nprint(\"\\nüìä Final Evaluation After Adaptation\")\nprint(classification_report(true_labels, final_pred_labels, target_names=label_encoder.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:26:44.693154Z","iopub.execute_input":"2025-07-14T18:26:44.693566Z","iopub.status.idle":"2025-07-14T18:27:06.318050Z","shell.execute_reply.started":"2025-07-14T18:26:44.693535Z","shell.execute_reply":"2025-07-14T18:27:06.317036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Federation and drift\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Split training data into N clients\ndef create_clients(X, y, num_clients=5):\n    client_data = []\n    data_per_client = len(X) // num_clients\n    for i in range(num_clients):\n        start = i * data_per_client\n        end = start + data_per_client\n        client_data.append((X[start:end], y[start:end]))\n    return client_data\n\nclients = create_clients(X_seq_balanced, y_seq_balanced, num_clients=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:27:22.502454Z","iopub.execute_input":"2025-07-14T18:27:22.503165Z","iopub.status.idle":"2025-07-14T18:27:22.522628Z","shell.execute_reply.started":"2025-07-14T18:27:22.503139Z","shell.execute_reply":"2025-07-14T18:27:22.521488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def federated_train_round(global_model, clients):\n    weights = []\n    for X_local, y_local in clients:\n        local_model = tf.keras.models.clone_model(global_model)\n        local_model.compile(optimizer='adam', loss=sparse_categorical_focal_loss(), metrics=['accuracy'])\n        local_model.set_weights(global_model.get_weights())\n        local_model.fit(X_local, y_local, epochs=1, batch_size=64, verbose=0)\n        weights.append(local_model.get_weights())\n    \n    # Average weights\n    new_weights = []\n    for weights_list_tuple in zip(*weights):\n        new_weights.append(np.mean(weights_list_tuple, axis=0))\n    \n    global_model.set_weights(new_weights)\n    return global_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:27:30.824229Z","iopub.execute_input":"2025-07-14T18:27:30.824529Z","iopub.status.idle":"2025-07-14T18:27:30.830878Z","shell.execute_reply.started":"2025-07-14T18:27:30.824505Z","shell.execute_reply":"2025-07-14T18:27:30.829993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"global_model = create_model(input_shape=X_seq_balanced.shape[1:], num_classes=num_classes)\nglobal_model.compile(optimizer='adam', loss=sparse_categorical_focal_loss(), metrics=['accuracy'])\n\nfor round_num in range(5):\n    print(f\"üåÄ Federated Training Round {round_num+1}\")\n    global_model = federated_train_round(global_model, clients)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:27:35.544230Z","iopub.execute_input":"2025-07-14T18:27:35.544531Z","iopub.status.idle":"2025-07-14T18:27:35.563212Z","shell.execute_reply.started":"2025-07-14T18:27:35.544508Z","shell.execute_reply":"2025-07-14T18:27:35.561899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import entropy\n\ndef kl_divergence(p, q):\n    p = np.asarray(p) + 1e-7  # smooth\n    q = np.asarray(q) + 1e-7\n    return entropy(p, q)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:27:39.860333Z","iopub.execute_input":"2025-07-14T18:27:39.860913Z","iopub.status.idle":"2025-07-14T18:27:39.866226Z","shell.execute_reply.started":"2025-07-14T18:27:39.860884Z","shell.execute_reply":"2025-07-14T18:27:39.865213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import entropy\nimport numpy as np\n\ndef detect_drift(X_window, baseline_mean):\n    # Reshape and flatten the window (if 3D)\n    if len(X_window.shape) == 3:\n        X_window = X_window.reshape(-1, X_window.shape[-1])\n    \n    current_mean = np.mean(X_window, axis=0)\n\n    # Ensure both vectors are non-negative (required for probability distributions)\n    baseline = np.clip(baseline_mean, a_min=1e-8, a_max=None)\n    current = np.clip(current_mean, a_min=1e-8, a_max=None)\n\n    # Normalize to make them proper probability distributions\n    p = baseline / np.sum(baseline)\n    q = current / np.sum(current)\n\n    # Compute KL Divergence safely\n    kl = entropy(p, q)\n\n    # If it still explodes due to NaN/Inf, fallback\n    if not np.isfinite(kl):\n        return 0.0  # or float('nan') if you want to track failures\n\n    return kl\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:27:44.428355Z","iopub.execute_input":"2025-07-14T18:27:44.428656Z","iopub.status.idle":"2025-07-14T18:27:44.435309Z","shell.execute_reply.started":"2025-07-14T18:27:44.428633Z","shell.execute_reply":"2025-07-14T18:27:44.434223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"baseline_window = X_test_seq[:1000]\nif len(baseline_window.shape) == 3:\n    baseline_window = baseline_window.reshape(-1, baseline_window.shape[-1])\n\nbaseline_mean = np.mean(baseline_window, axis=0)\nbaseline_mean = np.clip(baseline_mean, 1e-8, None)  # Prevent 0s\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:27:48.492063Z","iopub.execute_input":"2025-07-14T18:27:48.492860Z","iopub.status.idle":"2025-07-14T18:27:48.498117Z","shell.execute_reply.started":"2025-07-14T18:27:48.492823Z","shell.execute_reply":"2025-07-14T18:27:48.497333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DRIFT_THRESHOLD = 0.1\nwindow_size = 100\n\nfor i in range(0, len(X_test_seq), window_size):\n    X_window = X_test_seq[i:i+window_size]\n    drift_score = detect_drift(X_window, baseline_mean)\n    \n    print(f\"üìä Drift Score at batch {i//window_size}: {drift_score:.4f}\")\n    if drift_score > DRIFT_THRESHOLD:\n        print(\"‚ö†Ô∏è Concept Drift Detected! Consider retraining or fine-tuning the model.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:27:52.329152Z","iopub.execute_input":"2025-07-14T18:27:52.329442Z","iopub.status.idle":"2025-07-14T18:27:53.524521Z","shell.execute_reply.started":"2025-07-14T18:27:52.329420Z","shell.execute_reply":"2025-07-14T18:27:53.523423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save('adaptive_federated_ids_model.h5')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:28:07.865858Z","iopub.execute_input":"2025-07-14T18:28:07.866191Z","iopub.status.idle":"2025-07-14T18:28:07.943031Z","shell.execute_reply.started":"2025-07-14T18:28:07.866169Z","shell.execute_reply":"2025-07-14T18:28:07.942110Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\nimport json\n\njoblib.dump(label_encoder, 'label_encoder.pkl')\njoblib.dump(scaler, 'scaler.pkl')\n\nmetadata = {\n    \"sequence_length\": 10,\n    \"input_shape\": list(X_seq_balanced.shape[1:]),\n    \"num_classes\": num_classes\n}\nwith open(\"model_metadata.json\", \"w\") as f:\n    json.dump(metadata, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:28:10.385593Z","iopub.execute_input":"2025-07-14T18:28:10.386030Z","iopub.status.idle":"2025-07-14T18:28:10.409221Z","shell.execute_reply.started":"2025-07-14T18:28:10.386005Z","shell.execute_reply":"2025-07-14T18:28:10.408043Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import backend as K\n\ndef sparse_categorical_focal_loss(gamma=2.0, alpha=0.25):\n    def loss_fn(y_true, y_pred):\n        y_true = tf.cast(y_true, tf.int32)\n        y_true = tf.reshape(y_true, [-1])\n        y_pred = tf.clip_by_value(y_pred, K.epsilon(), 1. - K.epsilon())\n\n        y_pred = tf.gather(y_pred, y_true, axis=1, batch_dims=1)\n        loss = -alpha * (1 - y_pred) ** gamma * tf.math.log(y_pred)\n        return tf.reduce_mean(loss)\n    return loss_fn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:28:15.593355Z","iopub.execute_input":"2025-07-14T18:28:15.593633Z","iopub.status.idle":"2025-07-14T18:28:15.599944Z","shell.execute_reply.started":"2025-07-14T18:28:15.593614Z","shell.execute_reply":"2025-07-14T18:28:15.598991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = tf.keras.models.load_model(\n    'adaptive_federated_ids_model.h5',\n    compile=False\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:28:19.236663Z","iopub.execute_input":"2025-07-14T18:28:19.237040Z","iopub.status.idle":"2025-07-14T18:28:19.343956Z","shell.execute_reply.started":"2025-07-14T18:28:19.237013Z","shell.execute_reply":"2025-07-14T18:28:19.343047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss=sparse_categorical_focal_loss(),  # Redefined earlier\n    metrics=['accuracy']\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:28:22.052693Z","iopub.execute_input":"2025-07-14T18:28:22.053067Z","iopub.status.idle":"2025-07-14T18:28:22.064660Z","shell.execute_reply.started":"2025-07-14T18:28:22.053041Z","shell.execute_reply":"2025-07-14T18:28:22.063670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**WORKING**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport time\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.models import load_model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:28:25.568520Z","iopub.execute_input":"2025-07-14T18:28:25.569211Z","iopub.status.idle":"2025-07-14T18:28:25.573009Z","shell.execute_reply.started":"2025-07-14T18:28:25.569182Z","shell.execute_reply":"2025-07-14T18:28:25.572274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer='adam',\n    loss=sparse_categorical_focal_loss(),  # Your redefined focal loss\n    metrics=['accuracy']\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:28:27.749196Z","iopub.execute_input":"2025-07-14T18:28:27.749479Z","iopub.status.idle":"2025-07-14T18:28:27.760120Z","shell.execute_reply.started":"2025-07-14T18:28:27.749459Z","shell.execute_reply":"2025-07-14T18:28:27.759154Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Simulated real-time input stream using your test sequence\nWINDOW_SIZE = 10\nDELAY_BETWEEN_STEPS = 0.5  # seconds\n\nlabel_encoder = joblib.load(\"label_encoder.pkl\")  # for decoding predicted class index\nstream = X_test_seq  # 3D shape: (samples, window, features)\n\nprint(\"üöÄ Starting Real-Time IDS Simulation...\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:28:30.212380Z","iopub.execute_input":"2025-07-14T18:28:30.213122Z","iopub.status.idle":"2025-07-14T18:28:30.219130Z","shell.execute_reply.started":"2025-07-14T18:28:30.213095Z","shell.execute_reply":"2025-07-14T18:28:30.218324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy.stats import entropy\n\n# Baseline for drift detection\nbaseline_window = stream[:500].reshape(-1, stream.shape[2])\nbaseline_mean = np.mean(baseline_window, axis=0)\nbaseline_mean = np.clip(baseline_mean, 1e-8, None)\n\ndef detect_drift(X_window, baseline_mean):\n    current = np.mean(X_window.reshape(-1, X_window.shape[-1]), axis=0)\n    current = np.clip(current, 1e-8, None)\n\n    p = baseline_mean / np.sum(baseline_mean)\n    q = current / np.sum(current)\n\n    kl = entropy(p, q)\n    return kl if np.isfinite(kl) else 0.0\n\nDRIFT_THRESHOLD = 0.2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:29:18.841062Z","iopub.execute_input":"2025-07-14T18:29:18.841391Z","iopub.status.idle":"2025-07-14T18:29:18.848681Z","shell.execute_reply.started":"2025-07-14T18:29:18.841368Z","shell.execute_reply":"2025-07-14T18:29:18.847614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(stream)):\n    X_window = stream[i]  # Shape: (10, 42)\n    X_input = X_window.reshape(1, 10, 42)  # Shape: (1, 10, 42) ‚úÖ\n\n    # Prediction\n    y_pred = model.predict(X_input, verbose=0)\n    y_pred_label = np.argmax(y_pred)\n    label = label_encoder.inverse_transform([y_pred_label])[0]\n\n    # Drift detection\n    drift_score = detect_drift(X_window, baseline_mean)\n    drift_alert = drift_score > DRIFT_THRESHOLD\n\n    print(f\"üì∂ Time Step: {i}\")\n    print(f\"üîç Predicted Class: {label} ({'‚ö†Ô∏è Drift!' if drift_alert else '‚úÖ Stable'})\")\n    print(f\"üìä Drift Score: {drift_score:.4f}\")\n    print(\"-\" * 50)\n\n    time.sleep(0.3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:29:22.053067Z","iopub.execute_input":"2025-07-14T18:29:22.053385Z","iopub.status.idle":"2025-07-14T18:29:27.800654Z","shell.execute_reply.started":"2025-07-14T18:29:22.053362Z","shell.execute_reply":"2025-07-14T18:29:27.799430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport joblib\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load your test dataset (already uploaded)\ndf_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\n\n# Drop ID column if present\ndf_test.drop(['id'], axis=1, errors='ignore', inplace=True)\n\n# Load scaler and target label encoder\nscaler = joblib.load(\"scaler.pkl\")\nlabel_encoder = joblib.load(\"label_encoder.pkl\")\n\n# Encode target column\ndf_test['label_encoded'] = label_encoder.transform(df_test['attack_cat'])\n\n# üîç Re-encode categorical input columns (proto, service, state) just for this test\ncat_cols = ['proto', 'service', 'state']\nfor col in cat_cols:\n    le = LabelEncoder()\n    df_test[col] = le.fit_transform(df_test[col].astype(str))  # Fit-transform new just for inference\n\n# Select input features (exclude label columns)\nX_sim = df_test.drop(columns=['label', 'attack_cat', 'label_encoded'], errors='ignore')\ny_sim = df_test['label_encoded']\n\n# ‚úÖ Scale numerical data\nX_scaled_sim = scaler.transform(X_sim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:29:32.865867Z","iopub.execute_input":"2025-07-14T18:29:32.866629Z","iopub.status.idle":"2025-07-14T18:29:33.817977Z","shell.execute_reply.started":"2025-07-14T18:29:32.866601Z","shell.execute_reply":"2025-07-14T18:29:33.817256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_sliding_windows(X, y, window_size=10):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - window_size):\n        X_seq.append(X[i:i+window_size])\n        y_seq.append(y[i+window_size])\n    return np.array(X_seq), np.array(y_seq)\n\nX_seq_sim, y_seq_sim = create_sliding_windows(X_scaled_sim, y_sim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:29:36.627572Z","iopub.execute_input":"2025-07-14T18:29:36.627933Z","iopub.status.idle":"2025-07-14T18:29:37.327711Z","shell.execute_reply.started":"2025-07-14T18:29:36.627906Z","shell.execute_reply":"2025-07-14T18:29:37.326907Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow.keras.backend as K\n\n# Define the loss again\ndef sparse_categorical_focal_loss(gamma=2.0):\n    def loss(y_true, y_pred):\n        y_true = K.cast(y_true, 'int32')\n        y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n        pt = K.gather(y_pred, y_true, axis=-1, batch_dims=1)\n        return -K.mean((1 - pt) ** gamma * K.log(pt))\n    return loss\n\n# ‚úÖ Load without compiling first\nmodel = load_model(\"adaptive_federated_ids_model.h5\", compile=False)\n\n# ‚úÖ Manually compile\nmodel.compile(optimizer=Adam(), \n              loss=sparse_categorical_focal_loss(gamma=2.0), \n              metrics=[\"accuracy\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:29:40.639200Z","iopub.execute_input":"2025-07-14T18:29:40.639515Z","iopub.status.idle":"2025-07-14T18:29:40.761777Z","shell.execute_reply.started":"2025-07-14T18:29:40.639488Z","shell.execute_reply":"2025-07-14T18:29:40.760891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example: Predict on first 5 samples\nfor i in range(5):\n    sample = X_seq_sim[i].reshape(1, X_seq_sim.shape[1], X_seq_sim.shape[2])\n    pred = model.predict(sample)\n    pred_class = np.argmax(pred, axis=1)[0]\n    true_class = y_seq_sim[i]\n    print(f\"\\nüì∂ Time window: {i}\")\n    print(f\"üß† Predicted: {label_encoder.inverse_transform([pred_class])[0]}\")\n    print(f\"üõ°Ô∏è  Actual:    {label_encoder.inverse_transform([true_class])[0]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:29:43.846647Z","iopub.execute_input":"2025-07-14T18:29:43.846986Z","iopub.status.idle":"2025-07-14T18:29:44.654118Z","shell.execute_reply.started":"2025-07-14T18:29:43.846961Z","shell.execute_reply":"2025-07-14T18:29:44.653185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport joblib\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow.keras.backend as K\n\n# Load testing data\ndf_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\n\n# Drop ID\ndf_test.drop(['id'], axis=1, errors='ignore', inplace=True)\n\n# Sample 50 Normal and 50 Attack records\ndf_normal = df_test[df_test['attack_cat'] == 'Normal'].sample(50, random_state=42)\ndf_attack = df_test[df_test['attack_cat'] != 'Normal'].sample(50, random_state=42)\ndf_subset = pd.concat([df_normal, df_attack]).sample(frac=1, random_state=42)\n\n# Encode target\nlabel_encoder = joblib.load(\"label_encoder.pkl\")\ndf_subset['label_encoded'] = label_encoder.transform(df_subset['attack_cat'])\n\n# Label encode categorical columns used during training\nfor col in ['proto', 'service', 'state']:\n    le = LabelEncoder()\n    df_subset[col] = le.fit_transform(df_subset[col])\n\n# Separate features and labels\nX_sim = df_subset.drop(columns=['label', 'attack_cat', 'label_encoded'], errors='ignore')\ny_sim = df_subset['label_encoded'].values\n\n# Load scaler and transform\nscaler = joblib.load(\"scaler.pkl\")\nX_scaled = scaler.transform(X_sim)\n\n# Create LSTM sequences\ndef create_sequences(X, y, timesteps=10):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - timesteps):\n        X_seq.append(X[i:i+timesteps])\n        y_seq.append(y[i+timesteps])\n    return np.array(X_seq), np.array(y_seq)\n\nX_seq_sim, y_seq_sim = create_sequences(X_scaled, y_sim)\n\n# Define focal loss for loading\ndef sparse_categorical_focal_loss(gamma=2.0):\n    def loss(y_true, y_pred):\n        y_true = K.cast(y_true, 'int32')\n        y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n        pt = K.gather(y_pred, y_true, axis=-1, batch_dims=1)\n        return -K.mean((1 - pt) ** gamma * K.log(pt))\n    return loss\n\n# Load the model\nmodel = load_model(\"adaptive_federated_ids_model.h5\", compile=False)\nmodel.compile(optimizer=Adam(), \n              loss=sparse_categorical_focal_loss(gamma=2.0), \n              metrics=[\"accuracy\"])\n\n# Simulate predictions\nprint(\"üîÅ Real-time Prediction Simulation:\\n\")\n\nfor i in range(10):  # Test 10 time windows\n    sample = X_seq_sim[i].reshape(1, X_seq_sim.shape[1], X_seq_sim.shape[2])\n    pred = model.predict(sample, verbose=0)\n    pred_class = np.argmax(pred, axis=1)[0]\n    true_class = y_seq_sim[i]\n\n    pred_label = label_encoder.inverse_transform([pred_class])[0]\n    true_label = label_encoder.inverse_transform([true_class])[0]\n\n    print(f\"üì∂ Time Window {i+1}\")\n    print(f\"üß† Predicted: {'‚úÖ' if pred_label == true_label else '‚ùå'} {pred_label}\")\n    print(f\"üõ°Ô∏è  Actual:    {true_label}\")\n    print(\"-\" * 50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:29:49.189577Z","iopub.execute_input":"2025-07-14T18:29:49.189919Z","iopub.status.idle":"2025-07-14T18:29:51.368928Z","shell.execute_reply.started":"2025-07-14T18:29:49.189895Z","shell.execute_reply":"2025-07-14T18:29:51.368093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\n# Make sure y_seq_balanced is 1D integer labels (not one-hot encoded)\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# If y_seq_balanced is one-hot, convert back to integer labels\nif len(y_seq_balanced.shape) > 1:\n    y_seq_balanced_int = np.argmax(y_seq_balanced, axis=1)\nelse:\n    y_seq_balanced_int = y_seq_balanced\n\n# Compute class weights\nclass_weights = compute_class_weight(class_weight='balanced',\n                                     classes=np.unique(y_seq_balanced_int),\n                                     y=y_seq_balanced_int)\n\nclass_weights_dict = dict(enumerate(class_weights))\n\n# üîπ Model architecture\ndef create_multiclass_model(input_shape, num_classes):\n    model = Sequential([\n        LSTM(64, input_shape=input_shape, return_sequences=True),\n        Dropout(0.3),\n        LSTM(64),\n        Dropout(0.3),\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=['accuracy'])\n    return model\n\n# üîπ Re-train\nmodel = create_multiclass_model(input_shape=X_seq_balanced.shape[1:], num_classes=num_classes)\nmodel.fit(X_seq_balanced, to_categorical(y_seq_balanced_int), \n          epochs=10, batch_size=64, \n          validation_split=0.2, class_weight=class_weights_dict)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:29:54.821199Z","iopub.execute_input":"2025-07-14T18:29:54.821465Z","iopub.status.idle":"2025-07-14T18:29:54.845321Z","shell.execute_reply.started":"2025-07-14T18:29:54.821446Z","shell.execute_reply":"2025-07-14T18:29:54.844138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample a mix of correctly classified + adaptive drifted samples\ncorrect_X = []\ncorrect_y = []\n\nfor i in range(len(y_preds)):\n    if y_preds[i] == y_trues[i]:\n        correct_X.append(X_seq_sim[i])\n        correct_y.append(y_seq_sim[i])\n\n# Balance the dataset\ncombined_X = np.concatenate((adaptive_X, np.array(correct_X)[:len(adaptive_X)]))\ncombined_y = np.concatenate((np.argmax(adaptive_y, axis=1), np.array(correct_y)[:len(adaptive_X)]))\n\ncombined_y_cat = to_categorical(combined_y, num_classes=num_classes)\n\n# Compute class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(combined_y), y=combined_y)\nclass_weights_dict = dict(enumerate(class_weights))\n\n# Retrain\nmodel.fit(combined_X, combined_y_cat,\n          epochs=30,\n          batch_size=32,\n          class_weight=class_weights_dict,\n          callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n          verbose=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:29:59.265334Z","iopub.execute_input":"2025-07-14T18:29:59.265647Z","iopub.status.idle":"2025-07-14T18:29:59.287482Z","shell.execute_reply.started":"2025-07-14T18:29:59.265626Z","shell.execute_reply":"2025-07-14T18:29:59.286191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n\ndef create_better_model(input_shape, num_classes):\n    model = Sequential()\n    model.add(LSTM(128, return_sequences=True, input_shape=input_shape))\n    model.add(Dropout(0.3))\n    model.add(LSTM(64))\n    model.add(Dropout(0.3))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n    return model\n\n# Compile with focal loss or crossentropy\nmodel = create_better_model(input_shape=X_seq_sim.shape[1:], num_classes=num_classes)\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:30:03.077184Z","iopub.execute_input":"2025-07-14T18:30:03.077454Z","iopub.status.idle":"2025-07-14T18:30:03.169147Z","shell.execute_reply.started":"2025-07-14T18:30:03.077433Z","shell.execute_reply":"2025-07-14T18:30:03.168165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.losses import CategoricalCrossentropy\n\ndef sparse_categorical_focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        weights = alpha * tf.pow(1 - y_pred, gamma)\n        return tf.reduce_sum(weights * cross_entropy, axis=1)\n    return loss\n\nmodel.compile(optimizer='adam',\n              loss=sparse_categorical_focal_loss(gamma=2.0),\n              metrics=['accuracy'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\n# Convert back to integer class labels\ny_int = np.argmax(y_seq_balanced, axis=1)\n\n# Compute class weights\nclass_weights = compute_class_weight(class_weight='balanced',\n                                     classes=np.unique(y_int),\n                                     y=y_int)\nclass_weights_dict = dict(enumerate(class_weights))\n\n# Fit the model\nhistory = model.fit(X_seq_balanced, y_seq_balanced,\n                    epochs=30,\n                    batch_size=64,\n                    class_weight=class_weights_dict,\n                    validation_split=0.2,\n                    verbose=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ‚úÖ Fix misalignment\nmin_len = min(len(X_test_seq), len(y_test_cat))\nX_test_seq = X_test_seq[:min_len]\ny_test_cat = y_test_cat[:min_len]\n\n# ‚úÖ Now evaluate\nloss, acc = model.evaluate(X_test_seq, y_test_cat, verbose=0)\nprint(f\"‚úÖ Test Accuracy: {acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\ny_pred_probs = model.predict(X_test_seq)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = np.argmax(y_test_cat, axis=1)\n\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Simulate drifted samples\nadaptive_X = []\nadaptive_y = []\n\nfor i in range(len(X_seq_sim)):\n    sample = X_seq_sim[i].reshape(1, 10, X_seq_sim.shape[2])\n    pred = model.predict(sample, verbose=0)\n    pred_label = np.argmax(pred)\n    true_label = y_seq_sim[i]\n    \n    if pred_label != true_label:\n        adaptive_X.append(X_seq_sim[i])\n        adaptive_y.append(true_label)\n\n# Fine-tune on misclassified\nif len(adaptive_X) > 0:\n    adaptive_X = np.array(adaptive_X)\n    adaptive_y_cat = to_categorical(np.array(adaptive_y), num_classes=num_classes)\n    \n    print(f\"üîÅ Fine-tuning on {len(adaptive_X)} misclassified samples...\")\n    model.fit(adaptive_X, adaptive_y_cat, epochs=15, batch_size=32, verbose=1)\nelse:\n    print(\"‚úÖ No misclassified samples for fine-tuning.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"simulation\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport joblib\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\n# üîπ Load training data (replace with actual path if different)\ndf = pd.read_csv('/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv')\n\n# üîπ Drop unnecessary columns\ndf.drop(['id'], axis=1, errors='ignore', inplace=True)\n\n# üîπ Drop non-numeric features or encode them if needed\ncategorical_cols = df.select_dtypes(include=['object']).columns\ndf.drop(columns=categorical_cols, inplace=True)\n\n# üîπ Drop rows with NaNs\ndf.dropna(inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport joblib\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\n\n# üîπ Load training data (replace with actual path if different)\ndf = pd.read_csv('/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv')\n\n# üîπ Drop unnecessary columns\ndf.drop(['id'], axis=1, errors='ignore', inplace=True)\n\n# üîπ Drop non-numeric features or encode them if needed\ncategorical_cols = df.select_dtypes(include=['object']).columns\ndf.drop(columns=categorical_cols, inplace=True)\n\n# üîπ Drop rows with NaNs\ndf.dropna(inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_lstm_sequences(X, y, time_steps=10):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - time_steps):\n        X_seq.append(X[i:i+time_steps])\n        y_seq.append(y[i+time_steps])\n    return np.array(X_seq), np.array(y_seq)\n\nX_seq_sim, y_seq_sim = create_lstm_sequences(X_scaled, y_sim)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nimport tensorflow as tf\n\n# Define focal loss function (not invoked here)\ndef sparse_categorical_focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        weights = alpha * tf.pow(1 - y_pred, gamma)\n        return tf.reduce_sum(weights * cross_entropy, axis=1)\n    return loss\n\n# üîπ Step 1: Load without compiling\nmodel = load_model(\"adaptive_federated_ids_model.h5\", compile=False)\n\n# üîπ Step 2: Compile again\nmodel.compile(optimizer='adam',\n              loss=sparse_categorical_focal_loss(gamma=2.0),\n              metrics=['accuracy'])\n\nprint(\"‚úÖ Model loaded and recompiled with focal loss.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nimport joblib\n\n# Load part of training data\ndf_real = pd.read_csv('/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv')\n\n# Drop unnecessary columns\ndf_real.drop(['id'], axis=1, errors='ignore', inplace=True)\n\n# üîπ Encode target\nlabel_encoder = joblib.load('label_encoder.pkl')\ndf_real['label_encoded'] = label_encoder.transform(df_real['attack_cat'])\n\n# üîπ Keep only samples that are NOT \"normal\"\nnon_normal_df = df_real[df_real['attack_cat'] != 'Normal'].sample(n=100, random_state=42)\n\n# Add some normal samples too\nnormal_df = df_real[df_real['attack_cat'] == 'Normal'].sample(n=50, random_state=42)\n\n# Combine and shuffle\ndf_sim = pd.concat([non_normal_df, normal_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:31:15.212761Z","iopub.execute_input":"2025-07-14T18:31:15.213751Z","iopub.status.idle":"2025-07-14T18:31:15.562888Z","shell.execute_reply.started":"2025-07-14T18:31:15.213717Z","shell.execute_reply":"2025-07-14T18:31:15.562153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport joblib\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.utils import to_categorical\n\n# -------------------------------\n# üîπ Define Custom Loss Function\n# -------------------------------\n# Define focal loss again\ndef sparse_categorical_focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        weights = alpha * tf.pow(1 - y_pred, gamma)\n        return tf.reduce_sum(weights * cross_entropy, axis=1)\n    return loss\n\n# Re-compile the trained model if needed\nmodel.compile(optimizer='adam',\n              loss=sparse_categorical_focal_loss(gamma=2.0),\n              metrics=['accuracy'])\n\n# Save in robust format\nmodel.save(\"adaptive_federated_ids_model.keras\")  # or just \"saved_model/\"\n\n# -------------------------------\n# üîπ Load Model and Preprocessors\n# -------------------------------\nmodel = load_model(\"adaptive_federated_ids_model.h5\", \n                   custom_objects={'sparse_categorical_focal_loss': sparse_categorical_focal_loss(gamma=2.0)})\nscaler = joblib.load(\"scaler.pkl\")\nlabel_encoder = joblib.load(\"label_encoder.pkl\")\nnum_classes = len(label_encoder.classes_)\n\n# -------------------------------\n# üîπ Load and Preprocess Data\n# -------------------------------\ndf = pd.read_csv(\"UNSW_NB15_training-set.csv\")  # or testing-set\ndf.drop(['id'], axis=1, errors='ignore', inplace=True)\n\n# Take a subset with mixed attack types\nsubset = df[df['attack_cat'].isin(['Generic', 'Fuzzers', 'DoS', 'Normal'])].sample(5000, random_state=42)\n\n# Encode target labels\nsubset['label_encoded'] = label_encoder.transform(subset['attack_cat'])\n\n# Extract features and labels\nX_sim = subset.drop(columns=['label', 'attack_cat', 'label_encoded'], errors='ignore')\ny_sim = subset['label_encoded'].values\n\n# Label encode categorical features\ncategorical_cols = ['proto', 'service', 'state']\nfor col in categorical_cols:\n    le = LabelEncoder()\n    X_sim[col] = le.fit_transform(X_sim[col].astype(str))\n\n# Scale using saved scaler\nX_scaled = scaler.transform(X_sim)\n\n# -------------------------------\n# üîπ Create LSTM Sequences\n# -------------------------------\ndef create_sequences(X, y, window_size=10):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - window_size):\n        X_seq.append(X[i:i+window_size])\n        y_seq.append(y[i+window_size])\n    return np.array(X_seq), np.array(y_seq)\n\nX_seq_sim, y_seq_sim = create_sequences(X_scaled, y_sim)\n\n# -------------------------------\n# üîπ Real-Time Prediction & Adaptation\n# -------------------------------\ny_preds, y_trues = [], []\n\nprint(\"üì° Starting Real-Time Simulation...\\n\")\n\nfor i in range(len(X_seq_sim)):\n    sample = X_seq_sim[i].reshape(1, 10, X_seq_sim.shape[2])\n    pred = model.predict(sample, verbose=0)\n    pred_label = np.argmax(pred)\n    true_label = y_seq_sim[i]\n    \n    print(f\"üì∂ Time Window {i+1}\")\n    print(f\"üß† Predicted: {'‚úÖ' if pred_label == true_label else '‚ùå'} {label_encoder.inverse_transform([pred_label])[0]}\")\n    print(f\"üõ°Ô∏è  Actual:    {label_encoder.inverse_transform([true_label])[0]}\")\n    print(\"--------------------------------------------------\")\n    \n    y_preds.append(pred_label)\n    y_trues.append(true_label)\n\n# -------------------------------\n# üîπ Extract Misclassified Samples\n# -------------------------------\nadaptive_X = []\nadaptive_y = []\n\nfor i in range(len(y_preds)):\n    if y_preds[i] != y_trues[i]:\n        adaptive_X.append(X_seq_sim[i])\n        adaptive_y.append(y_seq_sim[i])\n\nadaptive_X = np.array(adaptive_X)\nadaptive_y = to_categorical(np.array(adaptive_y), num_classes=num_classes)\n\n# -------------------------------\n# üîπ Fine-tune the Model\n# -------------------------------\nif len(adaptive_X) > 0:\n    print(f\"\\nüß† Fine-tuning on {len(adaptive_X)} misclassified samples...\")\n    model.fit(adaptive_X, adaptive_y, epochs=5, batch_size=32, verbose=1)\nelse:\n    print(\"\\n‚úÖ No misclassifications ‚Äî no retraining needed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:31:23.695942Z","iopub.execute_input":"2025-07-14T18:31:23.696251Z","iopub.status.idle":"2025-07-14T18:31:23.859905Z","shell.execute_reply.started":"2025-07-14T18:31:23.696227Z","shell.execute_reply":"2025-07-14T18:31:23.858748Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FINAL WORK**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"!pip install -q tensorflow\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nimport joblib\nimport json\nfrom scipy.stats import entropy\nfrom collections import deque\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:30:17.112419Z","iopub.execute_input":"2025-07-14T18:30:17.112680Z","iopub.status.idle":"2025-07-14T18:30:17.117904Z","shell.execute_reply.started":"2025-07-14T18:30:17.112662Z","shell.execute_reply":"2025-07-14T18:30:17.117027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\n\n# ‚úÖ Load without compiling\nmodel = load_model(\"/kaggle/working/adaptive_federated_ids_model.h5\", compile=False)\n\n# ‚úÖ Now manually compile it (use correct loss)\nfrom tensorflow.keras.losses import CategoricalCrossentropy\n\nmodel.compile(optimizer='adam',\n              loss=CategoricalCrossentropy(),\n              metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:30:23.367345Z","iopub.execute_input":"2025-07-14T18:30:23.367639Z","iopub.status.idle":"2025-07-14T18:30:23.479189Z","shell.execute_reply.started":"2025-07-14T18:30:23.367617Z","shell.execute_reply":"2025-07-14T18:30:23.478373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optional: define this if your model used focal loss\ndef sparse_categorical_focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        weights = alpha * tf.pow(1 - y_pred, gamma)\n        return tf.reduce_sum(weights * cross_entropy, axis=1)\n    return loss\n\nmodel.compile(optimizer='adam',\n              loss=sparse_categorical_focal_loss(gamma=2.0),\n              metrics=['accuracy'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:30:26.666088Z","iopub.execute_input":"2025-07-14T18:30:26.667143Z","iopub.status.idle":"2025-07-14T18:30:26.677603Z","shell.execute_reply.started":"2025-07-14T18:30:26.667093Z","shell.execute_reply":"2025-07-14T18:30:26.676918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load encoder and scaler\nscaler = joblib.load('/kaggle/working/scaler.pkl')\nlabel_encoder = joblib.load('/kaggle/working/label_encoder.pkl')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:30:30.208730Z","iopub.execute_input":"2025-07-14T18:30:30.209630Z","iopub.status.idle":"2025-07-14T18:30:30.215662Z","shell.execute_reply.started":"2025-07-14T18:30:30.209600Z","shell.execute_reply":"2025-07-14T18:30:30.214571Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load some training data for simulation (must include attacks)\ndf = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\n\n# Drop columns not needed\ndf.drop(['id'], axis=1, errors='ignore', inplace=True)\n\n# Encode target\ndf['label_encoded'] = label_encoder.transform(df['attack_cat'])\n\n# Separate features and target\nX_sim = df.drop(columns=['label', 'attack_cat', 'label_encoded'], errors='ignore')\ny_sim = df['label_encoded'].values\n\n# Encode categorical features\nX_sim = pd.get_dummies(X_sim)\n\n# Align with training columns (optional)\nexpected_columns = scaler.feature_names_in_\nX_sim = X_sim.reindex(columns=expected_columns, fill_value=0)\n\n# Scale features\nX_scaled = scaler.transform(X_sim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:30:32.949192Z","iopub.execute_input":"2025-07-14T18:30:32.949482Z","iopub.status.idle":"2025-07-14T18:30:33.354998Z","shell.execute_reply.started":"2025-07-14T18:30:32.949460Z","shell.execute_reply":"2025-07-14T18:30:33.354279Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nwith open('/kaggle/working/model_metadata.json', 'r') as f:\n    metadata = json.load(f)\n\nprint(\"üìÑ Available keys in metadata:\", metadata.keys())\nprint(\"üìÑ Metadata content:\", json.dumps(metadata, indent=2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:30:39.967535Z","iopub.execute_input":"2025-07-14T18:30:39.967884Z","iopub.status.idle":"2025-07-14T18:30:39.986746Z","shell.execute_reply.started":"2025-07-14T18:30:39.967856Z","shell.execute_reply":"2025-07-14T18:30:39.985561Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"timesteps = 10  # Set based on your original model setup\nnum_classes = metadata.get('num_classes', 10)  # fallback to 10 if not present\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:30:45.361955Z","iopub.execute_input":"2025-07-14T18:30:45.362276Z","iopub.status.idle":"2025-07-14T18:30:45.381273Z","shell.execute_reply.started":"2025-07-14T18:30:45.362240Z","shell.execute_reply":"2025-07-14T18:30:45.380079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to create sequences\n# üîπ Load metadata (if not already loaded)\nimport json\n\nwith open('/kaggle/working/model_metadata.json', 'r') as f:\n    metadata = json.load(f)\n\n#timesteps = metadata['timesteps']\ntimesteps = 10\nnum_classes = metadata['num_classes']  # also useful\n\ndef create_sequences(X, y, timesteps):\n    X_seq, y_seq = [], []\n    for i in range(timesteps, len(X)):\n        X_seq.append(X[i - timesteps:i])\n        y_seq.append(y[i])\n    return np.array(X_seq), np.array(y_seq)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:30:48.538221Z","iopub.execute_input":"2025-07-14T18:30:48.538961Z","iopub.status.idle":"2025-07-14T18:30:48.557552Z","shell.execute_reply.started":"2025-07-14T18:30:48.538934Z","shell.execute_reply":"2025-07-14T18:30:48.556526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drift detection setup\nfrom scipy.spatial.distance import jensenshannon\ndrift_threshold = 0.35\nhistory_dist = None\ndrift_batches = 0\n\n# Adaptive training buffers\nadaptive_X = []\nadaptive_y = []\n\n# Simulate real-time window predictions\nfor i in range(len(X_seq_sim)):\n    X_input = X_seq_sim[i].reshape(1, timesteps, X_seq_sim.shape[2])\n    y_true = y_seq_sim[i]\n    \n    y_pred = model.predict(X_input, verbose=0)\n    pred_class = np.argmax(y_pred)\n    \n    # Print prediction\n    print(f\"\\nü™ü Time Window {i+1}\")\n    print(f\"üß† Predicted: {'‚úÖ ' if pred_class == y_true else '‚ùå '} {label_encoder.inverse_transform([pred_class])[0]}\")\n    print(f\"üõ°Ô∏è  Actual:    {label_encoder.inverse_transform([y_true])[0]}\")\n    \n    # Drift detection\n    current_dist = y_pred.flatten()\n    if history_dist is not None:\n        drift_score = jensenshannon(current_dist, history_dist)\n        if drift_score > drift_threshold:\n            print(f\"‚ö†Ô∏è Drift Detected! Drift Score: {drift_score:.4f}\")\n            adaptive_X.append(X_seq_sim[i])\n            adaptive_y.append(y_seq_sim[i])\n    history_dist = current_dist\n\n# ‚úÖ Adaptive Learning on misclassified or drifted samples\nif len(adaptive_X) > 0:\n    print(f\"\\nüîÅ Retraining on {len(adaptive_X)} adaptive samples due to drift...\")\n    adaptive_X = np.array(adaptive_X)\n    adaptive_y_cat = to_categorical(np.array(adaptive_y), num_classes=num_classes)\n    model.fit(adaptive_X, adaptive_y_cat, epochs=3, batch_size=32)\nelse:\n    print(\"\\n‚úÖ No concept drift detected that requires adaptation.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nunique, counts = np.unique(y_seq_sim, return_counts=True)\nlabel_map = {i: label_encoder.inverse_transform([i])[0] for i in range(len(counts))}\ndf_class_counts = pd.DataFrame({'Class': [label_map[i] for i in unique], 'Count': counts})\nprint(df_class_counts.sort_values('Count'))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"correct_attack_X = []\ncorrect_attack_y = []\n\nfor i in range(len(y_preds)):\n    if y_preds[i] == y_seq_sim[i] and y_seq_sim[i] != normal_class_index:  # Assuming 0 = Normal\n        correct_attack_X.append(X_seq_sim[i])\n        correct_attack_y.append(y_seq_sim[i])\n\n# Combine with misclassified ones\nadaptive_X_total = np.concatenate([adaptive_X, np.array(correct_attack_X)], axis=0)\nadaptive_y_total = np.concatenate([adaptive_y, to_categorical(correct_attack_y, num_classes=num_classes)], axis=0)\n\n# Retrain\nmodel.fit(adaptive_X_total, adaptive_y_total, epochs=15, batch_size=64, verbose=1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils import resample\n\ndef oversample_sequences(X_seq, y_seq):\n    df = pd.DataFrame({'y': y_seq})\n    full = []\n\n    for class_id in np.unique(y_seq):\n        idxs = df[df['y'] == class_id].index\n        X_class = X_seq[idxs]\n        y_class = y_seq[idxs]\n\n        if len(idxs) < 30:  # Set your balancing goal here\n            X_up, y_up = resample(X_class, y_class,\n                                  replace=True,\n                                  n_samples=30,\n                                  random_state=42)\n            full.append((X_up, y_up))\n        else:\n            full.append((X_class, y_class))\n\n    # Merge back\n    X_bal = np.concatenate([x for x, _ in full], axis=0)\n    y_bal = np.concatenate([y for _, y in full], axis=0)\n    return X_bal, y_bal\n\nX_balanced, y_balanced = oversample_sequences(X_seq_sim, y_seq_sim)\ny_balanced_cat = to_categorical(y_balanced, num_classes=num_classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nclass_weights_raw = compute_class_weight(class_weight='balanced',\n                                         classes=np.unique(y_balanced),\n                                         y=y_balanced)\n\nclass_weights = {i: w for i, w in enumerate(class_weights_raw)}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sparse_categorical_focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        weights = alpha * tf.pow(1 - y_pred, gamma)\n        return tf.reduce_sum(weights * cross_entropy, axis=1)\n    return loss\n\nmodel.compile(optimizer='adam',\n              loss=sparse_categorical_focal_loss(gamma=2.0),\n              metrics=['accuracy'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(X_balanced, y_balanced_cat,\n                    epochs=30,\n                    batch_size=64,\n                    class_weight=class_weights,\n                    validation_split=0.2,\n                    shuffle=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the unique labels present\npresent_classes = sorted(np.unique(y_seq_sim))\ntarget_names_filtered = [label_encoder.classes_[i] for i in present_classes]\n\nprint(\"üß™ Final Classification Report (After Balancing):\")\nprint(classification_report(y_seq_sim, y_pred, target_names=target_names_filtered))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def sparse_categorical_focal_loss(gamma=2.0, alpha=0.25):\n    def loss(y_true, y_pred):\n        y_pred = tf.clip_by_value(y_pred, 1e-9, 1.0)\n        cross_entropy = -y_true * tf.math.log(y_pred)\n        weights = alpha * tf.pow(1 - y_pred, gamma)\n        return tf.reduce_sum(weights * cross_entropy, axis=1)\n    return loss\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(optimizer='adam',\n              loss=sparse_categorical_focal_loss(gamma=2.0, alpha=0.25),\n              metrics=['accuracy'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U scikit-learn==1.3.2 imbalanced-learn==0.11.0\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#import os\n#os.kill(os.getpid(), 9) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport json\nimport joblib\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nfrom imblearn.over_sampling import SMOTE\n# Load model\nmodel = load_model(\"/kaggle/working/adaptive_federated_ids_model.h5\", compile=False)\n\n# Load scaler and label encoder\nscaler = joblib.load(\"/kaggle/working/scaler.pkl\")\nlabel_encoder = joblib.load(\"/kaggle/working/label_encoder.pkl\")\n\n# Load metadata\nwith open(\"/kaggle/working/model_metadata.json\", \"r\") as f:\n    metadata = json.load(f)\ntimesteps = metadata.get('timesteps', 10)\nnum_classes = metadata.get('num_classes', len(label_encoder.classes_))\n# If you had saved or uploaded preprocessed data\ndf = pd.read_csv(\"/kaggle/input/your-data-file.csv\")  # replace with your actual path if applicable\n\n# Apply preprocessing (replicate the exact steps used before)\n# For example:\ncategorical_columns = ['protocol_type', 'service', 'flag']\ndf[categorical_columns] = df[categorical_columns].astype(str)\ndf = pd.get_dummies(df, columns=categorical_columns)\n\nX = df.drop(\"label\", axis=1).values\ny = label_encoder.transform(df[\"label\"])\n\n# Scale the features\nX_scaled = scaler.transform(X)\n# Create LSTM-ready sequences\ndef create_sequences(X, y, timesteps):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - timesteps):\n        X_seq.append(X[i:i + timesteps])\n        y_seq.append(y[i + timesteps])\n    return np.array(X_seq), np.array(y_seq)\n\n# Select a slice of real-time simulation\nX_sim = X_scaled[:200]\ny_sim = y[:200]\n\nX_seq_sim, y_seq_sim = create_sequences(X_sim, y_sim, timesteps)\n# Flatten for SMOTE\nX_flattened = X_seq_sim.reshape((X_seq_sim.shape[0], -1))\n\n# Apply SMOTE\nsm = SMOTE(random_state=42)\nX_balanced_flat, y_balanced = sm.fit_resample(X_flattened, y_seq_sim)\n\n# Reshape back\nX_balanced = X_balanced_flat.reshape((-1, timesteps, X_seq_sim.shape[2]))\ny_balanced_cat = to_categorical(y_balanced, num_classes=num_classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**FINAL** 15/7/25","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import resample\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, clone_model\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom scipy.spatial.distance import cosine\n\ndf_train = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\ndf = pd.concat([df_train, df_test], ignore_index=True)\n\ndf['attack_cat'] = df['attack_cat'].fillna('Normal')\nlabel_encoder = LabelEncoder()\ndf['attack_cat'] = label_encoder.fit_transform(df['attack_cat'])\nnum_classes = len(label_encoder.classes_)\n\nX = df.drop(columns=['id', 'label', 'attack_cat'], errors='ignore')\ny = df['attack_cat']\nX = X.select_dtypes(include=[np.number])\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\ndef create_sequences(X, y, timesteps=10):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - timesteps):\n        X_seq.append(X[i:i+timesteps])\n        y_seq.append(y[i+timesteps])\n    return np.array(X_seq), np.array(y_seq)\n\ntimesteps = 10\nX_seq, y_seq = create_sequences(X_scaled, y, timesteps)\ny_seq_cat = to_categorical(y_seq, num_classes=num_classes)\n\nX_flat = X_seq.reshape((X_seq.shape[0], -1))\ndf_seq = pd.DataFrame(X_flat)\ndf_seq['label'] = y_seq\nmax_count = df_seq['label'].value_counts().max()\n\nbalanced = [resample(df_seq[df_seq['label'] == cls], replace=True, n_samples=max_count, random_state=42)\n            for cls in df_seq['label'].unique()]\ndf_bal = pd.concat(balanced)\n\nX_bal = df_bal.drop(columns='label').values.reshape((-1, timesteps, X_seq.shape[2]))\ny_bal = df_bal['label'].values\ny_bal_cat = to_categorical(y_bal, num_classes=num_classes)\n\ndef build_model(input_shape, num_classes):\n    model = Sequential([\n        LSTM(64, return_sequences=True, input_shape=input_shape),\n        Dropout(0.3),\n        LSTM(32),\n        Dense(64, activation='relu'),\n        Dropout(0.2),\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\nclients = 5\nX_splits = np.array_split(X_bal, clients)\ny_splits = np.array_split(y_bal_cat, clients)\n\nglobal_model = build_model((timesteps, X_seq.shape[2]), num_classes)\n\nfor rnd in range(3):\n    print(f\"\\nüîÅ Federated Round {rnd+1}\")\n    weights = []\n    for i in range(clients):\n        local_model = clone_model(global_model)\n        local_model.set_weights(global_model.get_weights())\n        local_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n        local_model.fit(X_splits[i], y_splits[i], epochs=3, verbose=0, batch_size=64)\n        weights.append(local_model.get_weights())\n\n    # Federated averaging\n    new_weights = []\n    for weights_list_tuple in zip(*weights):\n        new_weights.append(np.mean(weights_list_tuple, axis=0))\n    global_model.set_weights(new_weights)\n\ny_pred_probs = global_model.predict(X_bal, verbose=0)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = np.argmax(y_bal_cat, axis=1)\n\nprint(\"\\nüìä Final Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n\nprint(\"\\nüì° Real-Time Adaptive Monitoring:\")\nprevious_window = None\ndrift_threshold = 0.35\n\nfor i in range(50):\n    x = X_bal[i].reshape(1, timesteps, X_bal.shape[2])\n    pred_prob = global_model.predict(x, verbose=0)\n    pred_cls = np.argmax(pred_prob)\n    true_cls = y_true[i]\n    match = \"‚úÖ\" if pred_cls == true_cls else \"‚ùå\"\n    \n    print(f\"ü™ü {i+1}: üß† Predicted: {match} {label_encoder.classes_[pred_cls]} | üõ°Ô∏è Actual: {label_encoder.classes_[true_cls]}\")\n    \n    if previous_window is not None:\n        drift_score = cosine(previous_window, pred_prob.flatten())\n        if drift_score > drift_threshold:\n            print(f\"‚ö†Ô∏è Drift Detected! Score: {drift_score:.4f}\")\n    previous_window = pred_prob.flatten()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:50:54.282623Z","iopub.execute_input":"2025-07-14T18:50:54.283339Z","iopub.status.idle":"2025-07-14T19:08:40.898312Z","shell.execute_reply.started":"2025-07-14T18:50:54.283314Z","shell.execute_reply":"2025-07-14T19:08:40.897405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import resample\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, clone_model\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.optimizers import Adam\nfrom scipy.spatial.distance import cosine\nfrom tensorflow.keras import Input\n\n# ‚úÖ Load dataset\ndf_train = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\ndf = pd.concat([df_train, df_test], ignore_index=True)\n\ndf['attack_cat'] = df['attack_cat'].fillna('Normal')\nlabel_encoder = LabelEncoder()\ndf['attack_cat'] = label_encoder.fit_transform(df['attack_cat'])\nnum_classes = len(label_encoder.classes_)\n\nX = df.drop(columns=['id', 'label', 'attack_cat'], errors='ignore')\ny = df['attack_cat']\nX = X.select_dtypes(include=[np.number])\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\ndef create_sequences(X, y, timesteps=10):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - timesteps):\n        X_seq.append(X[i:i+timesteps])\n        y_seq.append(y[i+timesteps])\n    return np.array(X_seq), np.array(y_seq)\n\ntimesteps = 10\nX_seq, y_seq = create_sequences(X_scaled, y, timesteps)\ny_seq_cat = to_categorical(y_seq, num_classes=num_classes)\n\nX_flat = X_seq.reshape((X_seq.shape[0], -1))\ndf_seq = pd.DataFrame(X_flat)\ndf_seq['label'] = y_seq\nmax_count = df_seq['label'].value_counts().max()\n\nbalanced = [resample(df_seq[df_seq['label'] == cls], replace=True, n_samples=max_count, random_state=42)\n            for cls in df_seq['label'].unique()]\n# Instead of using the full `df_bal`\ndf_bal = df_bal.sample(n=8000, random_state=42)\n\n\nX_bal = df_bal.drop(columns='label').values.reshape((-1, timesteps, X_seq.shape[2]))\ny_bal = df_bal['label'].values\ny_bal_cat = to_categorical(y_bal, num_classes=num_classes)\n\ndef build_model(input_shape, num_classes):\n    model = Sequential([\n        Input(shape=input_shape),\n        LSTM(32, return_sequences=False),\n        Dropout(0.3),\n        Dense(32, activation='relu'),\n        Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer=Adam(learning_rate=0.005), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\nfrom sklearn.model_selection import StratifiedKFold\nclients = 8\nskf = StratifiedKFold(n_splits=clients, shuffle=True, random_state=42)\nsplits = list(skf.split(X_bal, y_bal))\nX_splits = [X_bal[train_idx] for train_idx, _ in splits]\ny_splits = [y_bal_cat[train_idx] for train_idx, _ in splits]\n\nglobal_model = build_model((timesteps, X_seq.shape[2]), num_classes)\n\nfor rnd in range(5):\n    print(f\"\\nüîÅ Federated Round {rnd+1}\")\n    weights = []\n    for i in range(clients):\n        local_model = clone_model(global_model)\n        local_model.set_weights(global_model.get_weights())\n        local_model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n        local_model.fit(X_splits[i], y_splits[i], epochs=5, verbose=0, batch_size=64)\n        weights.append(local_model.get_weights())\n\n    new_weights = []\n    for weights_list_tuple in zip(*weights):\n        new_weights.append(np.mean(weights_list_tuple, axis=0))\n    global_model.set_weights(new_weights)\n\ny_pred_probs = global_model.predict(X_bal, verbose=0)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = np.argmax(y_bal_cat, axis=1)\n\nprint(\"\\nüìä Final Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n\nprint(\"\\nüì° Real-Time Adaptive Monitoring:\")\nprevious_window = None\ndrift_threshold = 0.35\n\nfor i in range(50):\n    x = X_bal[i].reshape(1, timesteps, X_bal.shape[2])\n    pred_prob = global_model.predict(x, verbose=0)\n    pred_cls = np.argmax(pred_prob)\n    true_cls = y_true[i]\n    match = \"‚úÖ\" if pred_cls == true_cls else \"‚ùå\"\n    \n    print(f\"ü™ü {i+1}: üß† Predicted: {match} {label_encoder.classes_[pred_cls]} | üõ°Ô∏è Actual: {label_encoder.classes_[true_cls]}\")\n    \n    if previous_window is not None:\n        drift_score = cosine(previous_window, pred_prob.flatten())\n        if drift_score > drift_threshold:\n            print(f\"‚ö†Ô∏è Drift Detected! Score: {drift_score:.4f}\")\n    previous_window = pred_prob.flatten()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T11:17:09.168677Z","iopub.execute_input":"2025-07-15T11:17:09.168978Z","iopub.status.idle":"2025-07-15T11:20:26.602236Z","shell.execute_reply.started":"2025-07-15T11:17:09.168956Z","shell.execute_reply":"2025-07-15T11:20:26.601438Z"}},"outputs":[{"name":"stdout","text":"\nüîÅ Federated Round 1\n\nüîÅ Federated Round 2\n\nüîÅ Federated Round 3\n\nüîÅ Federated Round 4\n\nüîÅ Federated Round 5\n\nüìä Final Classification Report:\n                precision    recall  f1-score   support\n\n      Analysis       0.26      0.15      0.19       799\n      Backdoor       0.27      0.31      0.29       784\n           DoS       0.23      0.32      0.27       768\n      Exploits       0.28      0.06      0.10       765\n       Fuzzers       0.26      0.22      0.23       803\n       Generic       0.44      0.89      0.59       787\n        Normal       0.82      0.96      0.88       914\nReconnaissance       0.29      0.17      0.21       817\n     Shellcode       0.24      0.26      0.25       802\n         Worms       0.50      0.49      0.49       761\n\n      accuracy                           0.39      8000\n     macro avg       0.36      0.38      0.35      8000\n  weighted avg       0.37      0.39      0.36      8000\n\n\nüì° Real-Time Adaptive Monitoring:\nü™ü 1: üß† Predicted: ‚úÖ Reconnaissance | üõ°Ô∏è Actual: Reconnaissance\nü™ü 2: üß† Predicted: ‚ùå Worms | üõ°Ô∏è Actual: Shellcode\nü™ü 3: üß† Predicted: ‚ùå Normal | üõ°Ô∏è Actual: Backdoor\nü™ü 4: üß† Predicted: ‚ùå Generic | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 0.6499\nü™ü 5: üß† Predicted: ‚ùå Worms | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 0.5920\nü™ü 6: üß† Predicted: ‚ùå Generic | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.3701\nü™ü 7: üß† Predicted: ‚ùå DoS | üõ°Ô∏è Actual: Exploits\n‚ö†Ô∏è Drift Detected! Score: 0.4205\nü™ü 8: üß† Predicted: ‚ùå DoS | üõ°Ô∏è Actual: Backdoor\nü™ü 9: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\nü™ü 10: üß† Predicted: ‚ùå Backdoor | üõ°Ô∏è Actual: Exploits\nü™ü 11: üß† Predicted: ‚ùå Shellcode | üõ°Ô∏è Actual: Reconnaissance\nü™ü 12: üß† Predicted: ‚ùå Analysis | üõ°Ô∏è Actual: Shellcode\nü™ü 13: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 0.3564\nü™ü 14: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\nü™ü 15: üß† Predicted: ‚ùå Fuzzers | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 0.4701\nü™ü 16: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 0.9969\nü™ü 17: üß† Predicted: ‚ùå Shellcode | üõ°Ô∏è Actual: Exploits\n‚ö†Ô∏è Drift Detected! Score: 0.9980\nü™ü 18: üß† Predicted: ‚ùå Analysis | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.3506\nü™ü 19: üß† Predicted: ‚ùå Reconnaissance | üõ°Ô∏è Actual: Exploits\nü™ü 20: üß† Predicted: ‚ùå DoS | üõ°Ô∏è Actual: Analysis\nü™ü 21: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 0.9997\nü™ü 22: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.9983\nü™ü 23: üß† Predicted: ‚ùå Backdoor | üõ°Ô∏è Actual: Analysis\nü™ü 24: üß† Predicted: ‚ùå Reconnaissance | üõ°Ô∏è Actual: Analysis\nü™ü 25: üß† Predicted: ‚ùå Analysis | üõ°Ô∏è Actual: Fuzzers\nü™ü 26: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 0.6785\nü™ü 27: üß† Predicted: ‚ùå Worms | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 0.9780\nü™ü 28: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 0.7909\nü™ü 29: üß† Predicted: ‚ùå Worms | üõ°Ô∏è Actual: Exploits\n‚ö†Ô∏è Drift Detected! Score: 0.6577\nü™ü 30: üß† Predicted: ‚úÖ Backdoor | üõ°Ô∏è Actual: Backdoor\nü™ü 31: üß† Predicted: ‚ùå DoS | üõ°Ô∏è Actual: Normal\nü™ü 32: üß† Predicted: ‚ùå Shellcode | üõ°Ô∏è Actual: Reconnaissance\nü™ü 33: üß† Predicted: ‚ùå Shellcode | üõ°Ô∏è Actual: Analysis\nü™ü 34: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 0.9905\nü™ü 35: üß† Predicted: ‚úÖ Backdoor | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 0.9489\nü™ü 36: üß† Predicted: ‚ùå DoS | üõ°Ô∏è Actual: Exploits\nü™ü 37: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 0.9952\nü™ü 38: üß† Predicted: ‚ùå Fuzzers | üõ°Ô∏è Actual: Analysis\n‚ö†Ô∏è Drift Detected! Score: 0.9781\nü™ü 39: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 0.6002\nü™ü 40: üß† Predicted: ‚ùå Shellcode | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 0.5839\nü™ü 41: üß† Predicted: ‚ùå Generic | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 0.3999\nü™ü 42: üß† Predicted: ‚ùå Shellcode | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 0.4013\nü™ü 43: üß† Predicted: ‚ùå Generic | üõ°Ô∏è Actual: Exploits\nü™ü 44: üß† Predicted: ‚úÖ Worms | üõ°Ô∏è Actual: Worms\n‚ö†Ô∏è Drift Detected! Score: 0.4983\nü™ü 45: üß† Predicted: ‚ùå Shellcode | üõ°Ô∏è Actual: Worms\nü™ü 46: üß† Predicted: ‚úÖ Fuzzers | üõ°Ô∏è Actual: Fuzzers\nü™ü 47: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\nü™ü 48: üß† Predicted: ‚ùå Analysis | üõ°Ô∏è Actual: Exploits\nü™ü 49: üß† Predicted: ‚úÖ Shellcode | üõ°Ô∏è Actual: Shellcode\n‚ö†Ô∏è Drift Detected! Score: 0.4738\nü™ü 50: üß† Predicted: ‚úÖ Shellcode | üõ°Ô∏è Actual: Shellcode\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import resample\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model, clone_model\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom scipy.spatial.distance import cosine\nimport tensorflow as tf\n\ndf_train = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\ndf = pd.concat([df_train, df_test], ignore_index=True)\n\ndf['attack_cat'] = df['attack_cat'].fillna('Normal')\nlabel_encoder = LabelEncoder()\ndf['attack_cat'] = label_encoder.fit_transform(df['attack_cat'])\nnum_classes = len(label_encoder.classes_)\n\nX = df.drop(columns=['id', 'label', 'attack_cat'], errors='ignore')\ny = df['attack_cat']\nX = X.select_dtypes(include=[np.number])\nX_scaled = StandardScaler().fit_transform(X)\n\ndef create_sequences(X, y, timesteps=10):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - timesteps):\n        X_seq.append(X[i:i+timesteps])\n        y_seq.append(y[i+timesteps])\n    return np.array(X_seq), np.array(y_seq)\n\ntimesteps = 10\nX_seq, y_seq = create_sequences(X_scaled, y, timesteps)\ny_seq_cat = to_categorical(y_seq, num_classes=num_classes)\n\nX_flat = X_seq.reshape((X_seq.shape[0], -1))\ndf_seq = pd.DataFrame(X_flat)\ndf_seq['label'] = y_seq\nmax_samples_per_class = 800\n\nbalanced = [\n    resample(df_seq[df_seq['label'] == cls], \n             replace=True, \n             n_samples=max_samples_per_class, \n             random_state=42)\n    for cls in df_seq['label'].unique()\n]\ndf_bal = pd.concat(balanced).sample(n=8000, random_state=42)\n\nX_bal = df_bal.drop(columns='label').values.reshape((-1, timesteps, X_seq.shape[2]))\ny_bal = df_bal['label'].values\ny_bal_cat = to_categorical(y_bal, num_classes=num_classes)\n\ndef build_model(input_shape, num_classes):\n    inp = Input(shape=input_shape)\n    x = Bidirectional(LSTM(64, return_sequences=True))(inp)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = LSTM(32)(x)\n    x = BatchNormalization()(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    out = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=inp, outputs=out)\n    model.compile(optimizer=Adam(0.0007), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\nclients = 8\nskf = StratifiedKFold(n_splits=clients, shuffle=True, random_state=42)\nsplits = list(skf.split(X_bal, y_bal))\nX_splits = [X_bal[train_idx] for train_idx, _ in splits]\ny_splits = [y_bal_cat[train_idx] for train_idx, _ in splits]\n\nglobal_model = build_model((timesteps, X_seq.shape[2]), num_classes)\n\n\nfor rnd in range(7):  # More rounds = better convergence\n    print(f\"\\nüîÅ Federated Round {rnd+1}\")\n    weights = []\n    for i in range(clients):\n        local_model = clone_model(global_model)\n        local_model.set_weights(global_model.get_weights())\n        local_model.compile(optimizer=Adam(0.0007), loss='categorical_crossentropy', metrics=['accuracy'])\n        local_model.fit(X_splits[i], y_splits[i], epochs=7, batch_size=64, verbose=0)\n        weights.append(local_model.get_weights())\n  \n    new_weights = [np.mean(w, axis=0) for w in zip(*weights)]\n    global_model.set_weights(new_weights)\n\ny_pred_probs = global_model.predict(X_bal, verbose=0)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = np.argmax(y_bal_cat, axis=1)\nprint(\"\\nüìä Final Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n\nprint(\"\\nüì° Real-Time Adaptive Monitoring:\")\nprevious_window = None\ndrift_threshold = 0.35\ndrift_X, drift_y = [], []\n\nfor i in range(50):\n    x = X_bal[i].reshape(1, timesteps, X_bal.shape[2])\n    pred_prob = global_model.predict(x, verbose=0)\n    pred_cls = np.argmax(pred_prob)\n    true_cls = y_true[i]\n    match = \"‚úÖ\" if pred_cls == true_cls else \"‚ùå\"\n    \n    print(f\"ü™ü {i+1}: üß† Predicted: {match} {label_encoder.classes_[pred_cls]} | üõ°Ô∏è Actual: {label_encoder.classes_[true_cls]}\")\n    \n    if previous_window is not None:\n        drift_score = cosine(previous_window, pred_prob.flatten())\n        if drift_score > drift_threshold:\n            print(f\"‚ö†Ô∏è Drift Detected! Score: {drift_score:.4f}\")\n            drift_X.append(x)\n            drift_y.append(to_categorical([true_cls], num_classes=num_classes))\n    previous_window = pred_prob.flatten()\n\n    if len(drift_X) >= 10:\n        print(\"üîÑ Retraining global model on drifted data...\")\n        X_drift_batch = np.vstack(drift_X)\n        y_drift_batch = np.vstack(drift_y)\n        global_model.fit(X_drift_batch, y_drift_batch, epochs=2, verbose=0)\n        drift_X.clear()\n        drift_y.clear()\n\nglobal_model.save(\"federated_ids_model.h5\")\n\nimport pickle\nwith open(\"label_encoder.pkl\", \"wb\") as f:\n    pickle.dump(label_encoder, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T15:13:35.884789Z","iopub.execute_input":"2025-07-16T15:13:35.885052Z","iopub.status.idle":"2025-07-16T15:25:37.032461Z","shell.execute_reply.started":"2025-07-16T15:13:35.885018Z","shell.execute_reply":"2025-07-16T15:25:37.031723Z"}},"outputs":[{"name":"stderr","text":"2025-07-16 15:13:39.772401: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752678820.138193      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752678820.235218      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1752678840.345591      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1752678840.346386      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Federated Round 1\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1752678848.092638      97 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Federated Round 2\n\nüîÅ Federated Round 3\n\nüîÅ Federated Round 4\n\nüîÅ Federated Round 5\n\nüîÅ Federated Round 6\n\nüîÅ Federated Round 7\n\nüìä Final Classification Report:\n                precision    recall  f1-score   support\n\n      Analysis       0.66      0.59      0.62       800\n      Backdoor       0.61      0.64      0.62       800\n           DoS       0.61      0.55      0.58       800\n      Exploits       0.64      0.52      0.57       800\n       Fuzzers       0.67      0.51      0.58       800\n       Generic       0.59      0.93      0.72       800\n        Normal       0.96      0.98      0.97       800\nReconnaissance       0.69      0.52      0.59       800\n     Shellcode       0.66      0.77      0.71       800\n         Worms       0.96      1.00      0.98       800\n\n      accuracy                           0.70      8000\n     macro avg       0.70      0.70      0.70      8000\n  weighted avg       0.70      0.70      0.70      8000\n\n\nüì° Real-Time Adaptive Monitoring:\nü™ü 1: üß† Predicted: ‚ùå Analysis | üõ°Ô∏è Actual: Backdoor\nü™ü 2: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.7572\nü™ü 3: üß† Predicted: ‚úÖ Backdoor | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 0.6150\nü™ü 4: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.3953\nü™ü 5: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\nü™ü 6: üß† Predicted: ‚ùå Exploits | üõ°Ô∏è Actual: DoS\nü™ü 7: üß† Predicted: ‚ùå Analysis | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 0.5903\nü™ü 8: üß† Predicted: ‚ùå Generic | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 0.8538\nü™ü 9: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\n‚ö†Ô∏è Drift Detected! Score: 0.8680\nü™ü 10: üß† Predicted: ‚úÖ Exploits | üõ°Ô∏è Actual: Exploits\n‚ö†Ô∏è Drift Detected! Score: 0.6227\nü™ü 11: üß† Predicted: ‚ùå Exploits | üõ°Ô∏è Actual: Reconnaissance\nü™ü 12: üß† Predicted: ‚ùå Analysis | üõ°Ô∏è Actual: DoS\nü™ü 13: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.7292\nü™ü 14: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\n‚ö†Ô∏è Drift Detected! Score: 0.8138\nü™ü 15: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\nü™ü 16: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\n‚ö†Ô∏è Drift Detected! Score: 0.4091\nüîÑ Retraining global model on drifted data...\nü™ü 17: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 0.9849\nü™ü 18: üß† Predicted: ‚ùå Exploits | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 0.9708\nü™ü 19: üß† Predicted: ‚ùå Generic | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.9122\nü™ü 20: üß† Predicted: ‚úÖ Backdoor | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 0.9732\nü™ü 21: üß† Predicted: ‚úÖ Exploits | üõ°Ô∏è Actual: Exploits\n‚ö†Ô∏è Drift Detected! Score: 0.8251\nü™ü 22: üß† Predicted: ‚úÖ Worms | üõ°Ô∏è Actual: Worms\n‚ö†Ô∏è Drift Detected! Score: 0.9996\nü™ü 23: üß† Predicted: ‚úÖ Worms | üõ°Ô∏è Actual: Worms\nü™ü 24: üß† Predicted: ‚ùå Exploits | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 0.9997\nü™ü 25: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 0.9999\nü™ü 26: üß† Predicted: ‚ùå Shellcode | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.9992\nü™ü 27: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 0.8041\nüîÑ Retraining global model on drifted data...\nü™ü 28: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.8467\nü™ü 29: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 0.9997\nü™ü 30: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.9997\nü™ü 31: üß† Predicted: ‚ùå Fuzzers | üõ°Ô∏è Actual: Reconnaissance\nü™ü 32: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\nü™ü 33: üß† Predicted: ‚úÖ Shellcode | üõ°Ô∏è Actual: Shellcode\nü™ü 34: üß† Predicted: ‚úÖ Backdoor | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 0.7059\nü™ü 35: üß† Predicted: ‚ùå Normal | üõ°Ô∏è Actual: Shellcode\n‚ö†Ô∏è Drift Detected! Score: 0.5827\nü™ü 36: üß† Predicted: ‚úÖ Reconnaissance | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 0.7853\nü™ü 37: üß† Predicted: ‚ùå Generic | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 0.5731\nü™ü 38: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\n‚ö†Ô∏è Drift Detected! Score: 0.8548\nü™ü 39: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 0.9988\nü™ü 40: üß† Predicted: ‚ùå Backdoor | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nüîÑ Retraining global model on drifted data...\nü™ü 41: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.7065\nü™ü 42: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\nü™ü 43: üß† Predicted: ‚ùå DoS | üõ°Ô∏è Actual: Fuzzers\nü™ü 44: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\nü™ü 45: üß† Predicted: ‚úÖ Exploits | üõ°Ô∏è Actual: Exploits\n‚ö†Ô∏è Drift Detected! Score: 0.7637\nü™ü 46: üß† Predicted: ‚ùå Reconnaissance | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 0.4511\nü™ü 47: üß† Predicted: ‚ùå Exploits | üõ°Ô∏è Actual: Reconnaissance\nü™ü 48: üß† Predicted: ‚úÖ Worms | üõ°Ô∏è Actual: Worms\n‚ö†Ô∏è Drift Detected! Score: 0.9430\nü™ü 49: üß† Predicted: ‚úÖ Fuzzers | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 0.9401\nü™ü 50: üß† Predicted: ‚úÖ Worms | üõ°Ô∏è Actual: Worms\n‚ö†Ô∏è Drift Detected! Score: 0.9350\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import resample\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model, clone_model\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom scipy.spatial.distance import cosine\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nlr_callback = ReduceLROnPlateau(monitor='loss', patience=2, factor=0.5, verbose=1)\n\n\ndf_train = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\ndf = pd.concat([df_train, df_test], ignore_index=True)\n\n\ndf['attack_cat'] = df['attack_cat'].fillna('Normal')\nlabel_encoder = LabelEncoder()\ndf['attack_cat'] = label_encoder.fit_transform(df['attack_cat'])\nnum_classes = len(label_encoder.classes_)\n\nX = df.drop(columns=['id', 'label', 'attack_cat'], errors='ignore')\ny = df['attack_cat']\nX = X.select_dtypes(include=[np.number])\nX_scaled = StandardScaler().fit_transform(X)\n\n\ndef create_sequences(X, y, timesteps=10):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - timesteps):\n        X_seq.append(X[i:i+timesteps])\n        y_seq.append(y[i+timesteps])\n    return np.array(X_seq), np.array(y_seq)\n\ntimesteps = 10\nX_seq, y_seq = create_sequences(X_scaled, y, timesteps)\ny_seq_cat = to_categorical(y_seq, num_classes=num_classes)\n\n\nX_flat = X_seq.reshape((X_seq.shape[0], -1))\ndf_seq = pd.DataFrame(X_flat)\ndf_seq['label'] = y_seq\nmax_samples_per_class = 800\n\nbalanced = [\n    resample(df_seq[df_seq['label'] == cls], \n             replace=True, \n             n_samples=max_samples_per_class, \n             random_state=42)\n    for cls in df_seq['label'].unique()\n]\ndf_bal = pd.concat(balanced).sample(n=8000, random_state=42)\n\nX_bal = df_bal.drop(columns='label').values.reshape((-1, timesteps, X_seq.shape[2]))\ny_bal = df_bal['label'].values\ny_bal_cat = to_categorical(y_bal, num_classes=num_classes)\n\ndef build_model(input_shape, num_classes):\n    inp = Input(shape=input_shape)\n    x = Bidirectional(LSTM(128, return_sequences=True))(inp)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Bidirectional(LSTM(64))(x)\n    x = BatchNormalization()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(64, activation='relu')(x)\n\n    out = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=inp, outputs=out)\n    model.compile(optimizer=Adam(0.0007), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\nclients = 8\nskf = StratifiedKFold(n_splits=clients, shuffle=True, random_state=42)\nsplits = list(skf.split(X_bal, y_bal))\nX_splits = [X_bal[train_idx] for train_idx, _ in splits]\ny_splits = [y_bal_cat[train_idx] for train_idx, _ in splits]\n\nglobal_model = build_model((timesteps, X_seq.shape[2]), num_classes)\n\nfor rnd in range(20):  # More rounds = better convergence\n    print(f\"\\nüîÅ Federated Round {rnd+1}\")\n    weights = []\n    for i in range(clients):\n        local_model = clone_model(global_model)\n        local_model.set_weights(global_model.get_weights())\n        local_model.compile(optimizer=Adam(0.0007), loss='categorical_crossentropy', metrics=['accuracy'])\n        local_model.fit(X_splits[i], y_splits[i], epochs=7, batch_size=64, verbose=0, callbacks=[lr_callback])\n        weights.append(local_model.get_weights())\n   \n    new_weights = [np.mean(w, axis=0) for w in zip(*weights)]\n    global_model.set_weights(new_weights)\n\ny_pred_probs = global_model.predict(X_bal, verbose=0)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = np.argmax(y_bal_cat, axis=1)\nprint(\"\\nüìä Final Classification Report:\")\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n\n\nprint(\"\\nüì° Real-Time Adaptive Monitoring:\")\nprevious_window = None\ndrift_threshold = 0.35\ndrift_X, drift_y = [], []\n\nfor i in range(50):\n    x = X_bal[i].reshape(1, timesteps, X_bal.shape[2])\n    pred_prob = global_model.predict(x, verbose=0)\n    pred_cls = np.argmax(pred_prob)\n    true_cls = y_true[i]\n    match = \"‚úÖ\" if pred_cls == true_cls else \"‚ùå\"\n    \n    print(f\"ü™ü {i+1}: üß† Predicted: {match} {label_encoder.classes_[pred_cls]} | üõ°Ô∏è Actual: {label_encoder.classes_[true_cls]}\")\n    \n    if previous_window is not None:\n        drift_score = cosine(previous_window, pred_prob.flatten())\n        if drift_score > drift_threshold:\n            print(f\"‚ö†Ô∏è Drift Detected! Score: {drift_score:.4f}\")\n            drift_X.append(x)\n            drift_y.append(to_categorical([true_cls], num_classes=num_classes))\n    previous_window = pred_prob.flatten()\n\n    if len(drift_X) >= 10:\n        print(\"üîÑ Retraining global model on drifted data...\")\n        X_drift_batch = np.vstack(drift_X)\n        y_drift_batch = np.vstack(drift_y)\n        global_model.fit(X_drift_batch, y_drift_batch, epochs=2, verbose=0)\n        drift_X.clear()\n        drift_y.clear()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T18:20:19.471748Z","iopub.execute_input":"2025-07-15T18:20:19.472568Z","iopub.status.idle":"2025-07-15T19:00:01.622661Z","shell.execute_reply.started":"2025-07-15T18:20:19.472542Z","shell.execute_reply":"2025-07-15T19:00:01.621990Z"}},"outputs":[{"name":"stdout","text":"\nüîÅ Federated Round 1\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1752603630.600665     109 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\nüîÅ Federated Round 2\n\nüîÅ Federated Round 3\n\nüîÅ Federated Round 4\n\nüîÅ Federated Round 5\n\nüîÅ Federated Round 6\n\nüîÅ Federated Round 7\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 8\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 9\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 6: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 10\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 11\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 12\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 4: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 13\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 14\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 15\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 16\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 17\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 5: ReduceLROnPlateau reducing learning rate to 0.00017499999376013875.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 5: ReduceLROnPlateau reducing learning rate to 0.00017499999376013875.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 18\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 19\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 7: ReduceLROnPlateau reducing learning rate to 0.00017499999376013875.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüîÅ Federated Round 20\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 5: ReduceLROnPlateau reducing learning rate to 0.00017499999376013875.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nEpoch 3: ReduceLROnPlateau reducing learning rate to 0.0003499999875202775.\n\nüìä Final Classification Report:\n                precision    recall  f1-score   support\n\n      Analysis       0.99      0.99      0.99       800\n      Backdoor       0.98      0.99      0.99       800\n           DoS       0.99      0.99      0.99       800\n      Exploits       0.99      0.99      0.99       800\n       Fuzzers       1.00      1.00      1.00       800\n       Generic       1.00      1.00      1.00       800\n        Normal       1.00      1.00      1.00       800\nReconnaissance       1.00      1.00      1.00       800\n     Shellcode       1.00      1.00      1.00       800\n         Worms       1.00      1.00      1.00       800\n\n      accuracy                           1.00      8000\n     macro avg       1.00      1.00      1.00      8000\n  weighted avg       1.00      1.00      1.00      8000\n\n\nüì° Real-Time Adaptive Monitoring:\nü™ü 1: üß† Predicted: ‚úÖ Backdoor | üõ°Ô∏è Actual: Backdoor\nü™ü 2: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 3: üß† Predicted: ‚úÖ Backdoor | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 4: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.9999\nü™ü 5: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 6: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 0.9999\nü™ü 7: üß† Predicted: ‚úÖ Backdoor | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 0.9997\nü™ü 8: üß† Predicted: ‚úÖ Reconnaissance | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 9: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 10: üß† Predicted: ‚úÖ Exploits | üõ°Ô∏è Actual: Exploits\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 11: üß† Predicted: ‚úÖ Reconnaissance | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nüîÑ Retraining global model on drifted data...\nü™ü 12: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 13: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\nü™ü 14: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 15: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\nü™ü 16: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\nü™ü 17: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 18: üß† Predicted: ‚úÖ Fuzzers | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 19: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 20: üß† Predicted: ‚úÖ Backdoor | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 0.9999\nü™ü 21: üß† Predicted: ‚úÖ Exploits | üõ°Ô∏è Actual: Exploits\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 22: üß† Predicted: ‚úÖ Worms | üõ°Ô∏è Actual: Worms\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 23: üß† Predicted: ‚úÖ Worms | üõ°Ô∏è Actual: Worms\nü™ü 24: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 25: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nüîÑ Retraining global model on drifted data...\nü™ü 26: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 27: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 28: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 29: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 30: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 31: üß† Predicted: ‚úÖ Reconnaissance | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 0.9992\nü™ü 32: üß† Predicted: ‚úÖ Generic | üõ°Ô∏è Actual: Generic\n‚ö†Ô∏è Drift Detected! Score: 0.9999\nü™ü 33: üß† Predicted: ‚úÖ Shellcode | üõ°Ô∏è Actual: Shellcode\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 34: üß† Predicted: ‚úÖ Backdoor | üõ°Ô∏è Actual: Backdoor\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 35: üß† Predicted: ‚úÖ Shellcode | üõ°Ô∏è Actual: Shellcode\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nüîÑ Retraining global model on drifted data...\nü™ü 36: üß† Predicted: ‚úÖ Reconnaissance | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 37: üß† Predicted: ‚úÖ Fuzzers | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 38: üß† Predicted: ‚úÖ Analysis | üõ°Ô∏è Actual: Analysis\n‚ö†Ô∏è Drift Detected! Score: 0.9999\nü™ü 39: üß† Predicted: ‚úÖ Normal | üõ°Ô∏è Actual: Normal\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 40: üß† Predicted: ‚úÖ Reconnaissance | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 41: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 42: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\nü™ü 43: üß† Predicted: ‚úÖ Fuzzers | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 44: üß† Predicted: ‚úÖ DoS | üõ°Ô∏è Actual: DoS\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 45: üß† Predicted: ‚úÖ Exploits | üõ°Ô∏è Actual: Exploits\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 46: üß† Predicted: ‚úÖ Fuzzers | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 0.9996\nüîÑ Retraining global model on drifted data...\nü™ü 47: üß† Predicted: ‚úÖ Reconnaissance | üõ°Ô∏è Actual: Reconnaissance\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 48: üß† Predicted: ‚úÖ Worms | üõ°Ô∏è Actual: Worms\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 49: üß† Predicted: ‚úÖ Fuzzers | üõ°Ô∏è Actual: Fuzzers\n‚ö†Ô∏è Drift Detected! Score: 1.0000\nü™ü 50: üß† Predicted: ‚úÖ Worms | üõ°Ô∏è Actual: Worms\n‚ö†Ô∏è Drift Detected! Score: 1.0000\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.utils import resample\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Model, clone_model\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional, Input, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom scipy.spatial.distance import cosine\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\n\nlr_callback = ReduceLROnPlateau(monitor='loss', patience=2, factor=0.5, verbose=1)\n\n# ‚úÖ Load data\ndf_train = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_training-set.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/unsw-nb15/UNSW_NB15_testing-set.csv\")\n\n# ‚úÖ Preprocess\nfor df in [df_train, df_test]:\n    df['attack_cat'] = df['attack_cat'].fillna('Normal')\n\nlabel_encoder = LabelEncoder()\ndf_train['attack_cat'] = label_encoder.fit_transform(df_train['attack_cat'])\ndf_test['attack_cat'] = label_encoder.transform(df_test['attack_cat'])\nnum_classes = len(label_encoder.classes_)\n\ndef preprocess(df):\n    X = df.drop(columns=['id', 'label', 'attack_cat'], errors='ignore')\n    y = df['attack_cat']\n    X = X.select_dtypes(include=[np.number])\n    X_scaled = StandardScaler().fit_transform(X)\n    return X_scaled, y\n\nX_train, y_train = preprocess(df_train)\nX_test, y_test = preprocess(df_test)\n\n# ‚úÖ Sequence generation\ndef create_sequences(X, y, timesteps=10):\n    X_seq, y_seq = [], []\n    for i in range(len(X) - timesteps):\n        X_seq.append(X[i:i+timesteps])\n        y_seq.append(y[i+timesteps])\n    return np.array(X_seq), np.array(y_seq)\n\ntimesteps = 10\nX_train_seq, y_train_seq = create_sequences(X_train, y_train, timesteps)\nX_test_seq, y_test_seq = create_sequences(X_test, y_test, timesteps)\n\ny_train_seq_cat = to_categorical(y_train_seq, num_classes=num_classes)\ny_test_seq_cat = to_categorical(y_test_seq, num_classes=num_classes)\n\n# ‚úÖ Balance training set\nX_flat = X_train_seq.reshape((X_train_seq.shape[0], -1))\ndf_seq = pd.DataFrame(X_flat)\ndf_seq['label'] = y_train_seq\nmax_samples_per_class = 800\n\nbalanced = [\n    resample(df_seq[df_seq['label'] == cls], \n             replace=True, \n             n_samples=max_samples_per_class, \n             random_state=42)\n    for cls in df_seq['label'].unique()\n]\ndf_bal = pd.concat(balanced).sample(n=8000, random_state=42)\n\nX_bal = df_bal.drop(columns='label').values.reshape((-1, timesteps, X_train_seq.shape[2]))\ny_bal = df_bal['label'].values\ny_bal_cat = to_categorical(y_bal, num_classes=num_classes)\n\n\ndef build_model(input_shape, num_classes):\n    inp = Input(shape=input_shape)\n    x = Bidirectional(LSTM(128, return_sequences=True))(inp)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Bidirectional(LSTM(64))(x)\n    x = BatchNormalization()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.3)(x)\n    x = Dense(64, activation='relu')(x)\n    out = Dense(num_classes, activation='softmax')(x)\n    model = Model(inputs=inp, outputs=out)\n    model.compile(optimizer=Adam(0.0007), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n\nclients = 8\nskf = StratifiedKFold(n_splits=clients, shuffle=True, random_state=42)\nsplits = list(skf.split(X_bal, y_bal))\nX_splits = [X_bal[train_idx] for train_idx, _ in splits]\ny_splits = [y_bal_cat[train_idx] for train_idx, _ in splits]\n\nglobal_model = build_model((timesteps, X_bal.shape[2]), num_classes)\n\n\nfor rnd in range(10):\n    print(f\"\\nüîÅ Federated Round {rnd+1}\")\n    weights = []\n    for i in range(clients):\n        local_model = clone_model(global_model)\n        local_model.set_weights(global_model.get_weights())\n        local_model.compile(optimizer=Adam(0.0007), loss='categorical_crossentropy', metrics=['accuracy'])\n        local_model.fit(X_splits[i], y_splits[i], epochs=5, batch_size=64, verbose=0, callbacks=[lr_callback])\n        weights.append(local_model.get_weights())\n\n    new_weights = [np.mean(w, axis=0) for w in zip(*weights)]\n    global_model.set_weights(new_weights)\n\n# ‚úÖ Evaluate on true unseen test set\ny_pred_probs = global_model.predict(X_test_seq, verbose=0)\ny_pred = np.argmax(y_pred_probs, axis=1)\ny_true = y_test_seq\n\nprint(\"\\nüìä Final Classification Report (Unseen Test Set):\")\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n\n\nprint(\"\\nüì° Real-Time Adaptive Monitoring:\")\nprevious_window = None\ndrift_threshold = 0.35\ndrift_X, drift_y = [], []\n\nfor i in range(50):\n    x = X_test_seq[i].reshape(1, timesteps, X_test_seq.shape[2])\n    pred_prob = global_model.predict(x, verbose=0)\n    pred_cls = np.argmax(pred_prob)\n    true_cls = y_test_seq[i]\n    match = \"‚úÖ\" if pred_cls == true_cls else \"‚ùå\"\n\n    print(f\"ü™ü {i+1}: üß† Predicted: {match} {label_encoder.classes_[pred_cls]} | üõ°Ô∏è Actual: {label_encoder.classes_[true_cls]}\")\n\n    if previous_window is not None:\n        drift_score = cosine(previous_window, pred_prob.flatten())\n        if drift_score > drift_threshold:\n            print(f\"‚ö†Ô∏è Drift Detected! Score: {drift_score:.4f}\")\n            drift_X.append(x)\n            drift_y.append(to_categorical([true_cls], num_classes=num_classes))\n    previous_window = pred_prob.flatten()\n\n    if len(drift_X) >= 10:\n        print(\"üîÑ Retraining global model on drifted data...\")\n        X_drift_batch = np.vstack(drift_X)\n        y_drift_batch = np.vstack(drift_y)\n        global_model.fit(X_drift_batch, y_drift_batch, epochs=2, verbose=0)\n        drift_X.clear()\n        drift_y.clear()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-15T19:09:42.760306Z","iopub.execute_input":"2025-07-15T19:09:42.760611Z"}},"outputs":[{"name":"stdout","text":"\nüîÅ Federated Round 1\n\nüîÅ Federated Round 2\n\nüîÅ Federated Round 3\n\nüîÅ Federated Round 4\n\nüîÅ Federated Round 5\n\nüîÅ Federated Round 6\n\nüîÅ Federated Round 7\n","output_type":"stream"}],"execution_count":null}]}